\section{Real programming languages}

In this section, we cover what programming languages do.

\subsection{Methodology}

We considered a variety of real world programming languages (with suitable libraries).
For each system, we considered the following questions:

\begin{enumerate}

\item
Does this programming language allow users to provide distinct types for scalar, vector and matrix variables?

\item
Are the multiplication, division and transposition operators defined for all possible types of variables?

\item
Can we use these multiplication, division and transposition operators to construct expressions that conform to the standard way of spelling them in numerical linear algebra and have the expected output? (For example, does \verb|x'*y| compute the inner product of \verb|x| and \verb|y|, producing a scalar?)

\item
Are standard identities in numerical linear algebra expressible in code form?

\end{enumerate}

\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\cline{2-11}
\multicolumn{1}{l|}{} & \multicolumn{10}{l|}{ELEMENTARY BEHAVIORS}\tabularnewline
\cline{2-11}
\multicolumn{1}{l|}{} & A{*}B & A{*}b & 1{*}a & a{*}1 & a{*}b & a{*}B & A' & a' & A/B & 1/1\tabularnewline
\hline
Julia & M & V & V & V & E & M{*} & M & M & M & S\tabularnewline
\hline
Lua/Torch & M & V & \textbf{E} & V & S & E & M & E & E & S\tabularnewline
\hline
Mathematica & M & V & V & V & E & V & M & V & M & S\tabularnewline
\hline
Matlab & M & - & - & - & - & - & M & - & M & -\tabularnewline
\hline
NumPy & M & V & V & V & S & M & M & V & E & S\tabularnewline
\hline
Python 3.5 & M & V & V & V & S & M & M & V & E & S\tabularnewline
\hline
R & M & M & - & - & E & M{*} & M & M & M & -\tabularnewline
\hline
\end{tabular}

M: Matrix, V: Vector, S: Scalar, E: Error, M{*}: matrix only if second
argument is 1xN, otherwise error, -: Concept does not exist

\textbf{Bold} : Most people consider this wrong behavior

\begin{tabular}{|l|l|l|l|l|l|l|l|}
\cline{2-8}
\multicolumn{1}{l|}{} & \multicolumn{4}{l|}{EXPRESSIONS} & \multicolumn{3}{l|}{IDENTITIES}\tabularnewline
\cline{2-8}
\multicolumn{1}{l|}{} & a'{*}b & a{*}b' & x'{*}A{*}y & x'{*}A{*}x/x'{*}x & (x{*}y'){*}z = x{*}(y'{*}z) & x''=x & (A{*}x)'=(x'{*}A')\tabularnewline
\hline
Julia & \textbf{V} & M & \textbf{V} & \textbf{E} & \textbf{E} & F & T\tabularnewline
\hline
Lua/Torch & S & \textbf{S} & E & \textbf{E} & \textbf{E} & T & \textbf{F}\tabularnewline
\hline
Mathematica & S & \textbf{S} & S & S & V & T & T\tabularnewline
\hline
Matlab & M & M & M & M & M & T & T\tabularnewline
\hline
NumPy & S & \textbf{S} & S & S & V & T & T\tabularnewline
\hline
Python 3.5 & S & \textbf{S} & S & S & V & T & T\tabularnewline
\hline
R & M & M & M & M & M & \textbf{F} & T\tabularnewline
\hline
\end{tabular}

T: True, F: False



\subsection{Eigen}

Eigen is a C++ library for high performance numerical linear
algebra.\cite{eigenweb}\footnote{The version of Eigen tested was v3.2.6,
compiled with clang with Apple LLVM v7.0.2. Eigen supports vectors and
matrices of fixed sizes (known at compile time) and dynamic sizes (known only
at run time). Here, we consider only the dynamic classes, which have 
the most intricate semantics.
}

Eigen heavily exploits C++ expression templates to provide flexibility
resulting from type polymorphism. Notably, it uses special templates (meant for
internal use) to capture the result of intermediate expressions, performing
what computer scientists call ``lazy evaluation''. For example, the result of
the expression \verb|x.transpose()|, performing the transpose of a vector
\verb|x| (of type \verb|VectorXd|) is neither a vector \verb|VectorXd| nor a
matrix \verb|MatrixXd|, but rather a special type, \verb|Transpose<|
\verb|VectorXd>|, that can be assigned to a new \verb|VectorXd| variable or a
new \verb|MatrixXd| variable. Similar types are used to capture and build other
intermediate expressions, such as \verb|ProductReturnType<MatrixXd, VectorXd>|
for the matrix-vector product \verb|A*x|.  This ingenious strategy of
determining the output type only at assignment time is made possible in a
static language like C++, where users are expected to specify the output type
of a variable when declared.\footnote{Strictly speaking, C++ also allows for
run-time polymorphism, where the output type is only determined at run time.
However, the mechanisms for doing so are cumbersome and in most cases produce
significant performance slowdowns.} The result is that in almost all cases,
users can choose to imagine the result of most linear algebra expressions as
vectors or matrices as they so choose.

Nevertheless, Eigen's strategy is not perfect. The heavy use of internal
classes to represent the output of computations means that both assignment and
equality testing in Eigen must be overloaded with new definitions to meet user
expectations. Oftentimes, the validity of an assignment statement can only be
determined at run time, resulting in overhead due to run time checks for
compatible shape and so on. Many new methods have to be introduced to overload
C++'s assignment and equality testing operators. Furthermore, the assignment
logic of vector transpose is ambiguous and may lead to subtle logic errors
because the result of assignment may violate user expectations of the
transitivity of equality. In Eigen, the result of \verb|x.transpose()| can be
assigned to either a \verb|VectorXd| variable or \verb|MatrixXd| variable,
i.e.\ , both of the following statements are valid:

\begin{verbatim}

VectorXd y = x.transpose();
MatrixXd Y = x.transpose();

\end{verbatim}
%
However, the shapes of \verb|y| and \verb|Y| differ: the former has $n$ rows and 1 column\footnote{Eigen defaults to column vectors and employs an implicit trailing singleton dimension rule similar to \sc{MATLAB}'s.} whereas the latter has 1 row and $n$ columns. In other words, the type of the variable being assigned to determines whether transposing a vector is the identity operation or produces a row matrix!

We find that unexpected behavior may result from the logic above. For example, the identity $(Ax)^\prime = x^\prime A^\prime$ may or may not hold depending on whether the expressions on either side have been assigned to variables, and if so, which variables:
%
\begin{verbatim}

//The type of (A*x).transpose() is
//  Transpose<GeneralProduct<MatrixXd, VectorXd>>
VectorXd v1 = (A*x).transpose();
MatrixXd m1 = (A*x).transpose();

//The type of x.transpose() * A.transpose() is
//  ProductReturnType<Transpose<VectorXd>, Transpose<MatrixXd>>
VectorXd v2 = x.transpose() * A.transpose();
MatrixXd m2 = x.transpose() * A.transpose();

assert ((A*x).transpose() == x.transpose() * A.transpose()); //true
assert (v1 == v2); // true
assert (m1 == m2); // true
assert (v1 == m2); // assertion failure
assert (m1 == v2); // assertion failure

\end{verbatim}
%
The equality comparison operator \verb|==| must be suitably defined to understand when two values with complicated and different types can test equal to each other in the three cases above that do not trigger the assertion failure. Furthermore, the result of assigning \verb|(A*x).transpose()| to  \verb|v1| produces a column vector effectively equal to \verb|A*x|,
but the assignment to \verb|m1| produces a row matrix that is the transpose of \verb|v1|.
A similar result arises for \verb|v2| and \verb|m2|, with the result that the
equality holds only when neither side is assigned to a vector or matrix
variable, or when both sides are assigned to variables of the same type. In
this case, users may be confused as to why certain quantities may not be equal
to each other as a result of subtle reasoning errors about the types of various
expressions.


\subsection{Julia}

Julia is a dynamic for technical computing, whose base library features
generous functionality for linear algebraic operations.~\cite{Bezanson2012}

Julia's multiplication operator, \verb|*|, implements matrix--matrix and
matrix--vector products, as well as scalings of vectors and matrices, on top of
ordinary scalar multiplication.
Julia's division \verb|x/y| is equivalent to multiplication of \verb|x| by the
inverse of \verb|y| on the right. For non-scalar \verb|y|, the inverse is
generalized to matrix inverses and pseudoinverses (in the least-squares sense)
where appropriate.
Julia's transposition operator, \verb|'|, promotes vectors to column matrices
before transposing them. Thus we have that vector transposition is not
idempotent, \verb|x''|$\ne$\verb|x|. 

Julia's current semantics lead to some unexpected behavior. Expressions like
\verb|a'*b| for the inner product produce a vector of length 1, which do not
have the semantics of scalars. Other identities, like $(xy^\prime)z =
x(y^\prime z)$, also fail to hold in code, since \verb|x*(y'z)| becomes the
product of a vector with a 1-vector, which is not defined.



\subsection{R}

R is a widely used language for statistical computing.~\cite{Rlang}.
R's statistical features work primarily on data frames, time series and vectors.
R also has matrices, but no true scalars.
Its matrix multiplication operator is \verb|%*%| and its transposition operator
is \verb|t()|.
R's transposition operator promotes vectors to column matrices before
transposition, producing a row matrix.
Like Python, \verb|%*%| is also defined on two vectors to be the dot product.
\verb|%*%| also permits a vector--matrix product, where it implicitly
transposes the vector into a row matrix before doing an ordinary matrix
multiplication.
Extraneous singleton dimensions can be removed with \verb|drop()|.

R's semantics of vectors and matrices are simple to reason about.
Perhaps the only behavior that some users may find surprising is that
transposition is not idempotent on vectors.
Nevertheless, we've already seen precedent in the mathematical literature
that does...
\todo{cite}
