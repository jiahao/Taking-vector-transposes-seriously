\section{Summary of historical development}

The expectations of users today, and the conflict resulting in the question
of what the vector transpose does, can be traced back to the convoluted history
of present day numerical linear algebra, and the uneasy coexistence of two
incompatible schools of thought.

Let's ask which came first - the vector or the matrix? The answer depends on
which school you belong to. Many modern textbooks \todo{check Strang} introduce
vectors first as ordered sets or $n$-tuples with the algebraic structure of
vector spaces, with matrices later as describing linear transformations between
vectors.
However, in older textbooks, matrices came first, usually as an abstraction of
linear systems of equations. Vectors, if mentioned at all, are described as
derived in terms of rows and columns of matrices. In this school, it is common
to think of ``row/column vector'' as synonymous with ``row/column matrix''.

In the ``vectors first'' school, it makes sense to consider vectors as ``flat'',
lacking any intrinsic characteristics of row or columns in their shapes.
In the ``matrices first'' school, it makes sense that vectors must have intrinsic
``rowness'' or ``columness''.

Perhaps forgotten in the history of linear algebra is the role of vector analysis
in the development of the ``vectors first'' school. From quaternions we developed
the modern notions of vectors in three-dimensional space (and later, four-dimensional
space when general relativity was developed). Separately, we also saw massive
interest amongst mathematicians on algebras beyond the reals and complexes,
leading to the development of modern (abstract) algebra. The notion of vectors
in $n$-dimensional space came much later, when the vector space structure of
hypercomplex numbers was better understood.



\section{A timeline of the development of key ideas}

In this section we examine the historical development of linear algebra in
detail, tracing how the concepts of vectors, matrix multiplication and
matrix transposition were introduced and developed, leading to the modern day
presentation of matrices and vectors, their implementation in contemporary
programming languages, and the modern notion of types in programming language
theory.

A full development of the history of linear algebra and programming languages is
far beyond the scope of this paper; instead, we point interested readers to
noteworthy secondary sources~\cite{Parshall1985,Kleiner2007,Higham2009}.

Add Katz1995, Katz2009 \S 21.4, pp. 740-751 in 3/e.
Dorier1995, Moore1995

J. V. Collins, “An Elementary
Exposition of Grassmann’s Ausdehnungslehre, or Theory
of Extension,” American Mathematical Monthly 6
(1899), 193–198, 261–266, 297–301, and 7 (1900), 31–35,
163–166, 181–187, 207–214, 253–258, and more recently
in Desmond Fearnley-Sander, “Hermann Grassmann and
the Creation of Linear Algebra,” American Mathematical
Monthly 86 (1979), 809–817.

\subsection{Other secondary sources}


Note that we are not the first to point out the ``vectors first'' and ``matrices first'' schools.
See:

\begin{quote}
In 1888, Italian mathematician Giuseppe Peano formalized the first modern
definition of vector spaces and linear transformations. Peano's formal
definition of vector spaces did not gain much attention or popularity until
1918 when Weyl ``essentially repeated Peano's axioms'' in his book
Space-Time-Matter, and articulated an important relationship between a
``systems view'' and a ``vector spaces and transformations view'' of linear
algebra. Weyl then brings the subject of linear algebra full circle, pointing
out that by considering the coefficients of the unknowns in a system of linear
equations in $n$ unknowns as vectors, `our axioms characterize the basis of our
operations in the theory of linear equations'. (Katz, 1995, p. 204).

-- from \url{http://sigmaa.maa.org/rume/crume2010/Archive/Larson.pdf}

Katz, V.J. (1995) Historical Ideas in teaching Linear Algebra. In F. Swetz, J. Fauvel, O. Bekken,
B. Johansson, and V. Katz (Eds.), Learn from the Masters (pp. 189-206). Washington,
DC: Mathematical Association of America
\todo{Requested from library}
\end{quote}

Katz himself also discusses the history of linalg in terms of a matrix timeline and a vector timeline.


\subsection{The outer product}

\todo{Put this into timeline again.}

In contrast with the quadratic and bilinear forms, the outer product evolved separately into its modern form. The outer product $u v^\prime$ is closely related to Gibbs's notion of dyad, espoused in his lectures on vector analysis \cite{Gibbs1881,Wilson1901}.

The outer product is a misfit in the original list given in the introduction --- it is the only quantity which users expect to be a matrix, rather than a scalar.

First we need to address the fact that the outer product has two meanings in linear algebra. One sense is synonymous with the exterior product or wedge product, and was the original ``\textit{äußeres Produkt}'' of Grassmann's \textit{Ausdehnungslehre}.~\cite{Grassmann1844,Grassmann1862,Grassmann1995,Grassmann2000} The other sense is the tensor product of two vectors, which first appeared in Einstein and Grossmann's 1913 paper introducing general relativity,~\cite{Einstein1913} where they write

Die de gewöhnlichen Vektoranalsysis entnommenen Bezeichnungen ,,äußeres und inneres Produkt`` rechtfertigen sich, weil jene Operationen sich letzten Endes als besondere Fälle der hier betrachteten ergeben. The designations ``inner and outer product'', which are taken from ordinary vector analysis, are justified because, when all is said and done, those operations prove to be special cases of the operations considered here.~\cite[T. II, p. 26]{Einstein1913,Einstein1996}


\todo{Did Jordan, 1927 in the quantum mechanics papers do this too?}

The term ``outer product'' in this sense was introduced into the English literature in books about general relativity, such as in...

The term ``outer product'' appears to have been rediscovered by Ken Iverson in his seminal publications about APL.\@ In ``\textit{A Programming Language}''~\cite{Iverson1962book},

We should note that the ``outer product'' in the sense of tensor product has existed in many different forms throughout the mathematical literature, and has been called many different names and has many different notations. The oldest mention we could find of this concept is Grassmann's indeterminate product.

...

Grossmann cited Ricci and Levi-Civita's paper on tensor analysis,~\cite{Ricci1900} where the multiplication of two ``systèmes covariants (covariant systems)'' produced what was simply called a ``produit (product)''~\cite[p. 133]{Ricci1900} and is annotated as ``(tensor) product'' in a translation by Hermann.~\cite[p. 28]{Hermann1975}.

Apparently independently, Gibbs's dyads...



\paragraph{Caspar (1797)}

Coordinate systems? (As described in \cite{Katz1995})

On the Analytical Representation of Direction

see translation in David Smith, A Source Book in Mathematics

\paragraph{Cauchy (1829)}

Possibly the first paper to couch eigenvalue problems in terms of matrices (as
opposed to differential equations and eigenfunctions).

Symmetric matrices have real eigenvalues.

Well, actually in quadratic forms

Does he mention eigenvectors?

Katz, Ref. 1

\paragraph{Hamilton (1841)}

Clearly introduced the notion of n-tuples, which he called polyplets


Katz1995, p 202



\paragraph{Eisenstein (1844)~\cite{Eisenstein1844}}

Credited with the use of a single letter to refer to a matrix in its algebraic relations in \cite{Dorier1995}.

\cite[p. 327]{Eisenstein1844} in a lemma assigns the letter $\boldsymbol S$ to a ,,lineären Systems``
(``linear system'')
\[
\begin{Bmatrix}
\alpha, & \alpha^\prime, & \alpha^{\prime\prime} \\
\beta, & \beta^\prime, & \beta^{\prime\prime} \\
\gamma, & \gamma^\prime, & \gamma^{\prime\prime} \\
\end{Bmatrix} = \boldsymbol S
\]
which is clearly meant to represent the coefficients on the left hand side of a
linear system of equations.

Furthermore by considering the composition of  ,,der associirten Formen`` (``the associated [bilinear] forms''),
Eisenstein derived the formula for matrix multiplication by substitution of variables
in \S 8. II, pp. 356--362.


\paragraph{Grassmann (1844) - Ausdehnungslehre I~\cite{Grassmann1844,Grassmann1995}}

The contribution of Grassmann to the user psychology of linear algebra are not well understood, even today. In this section, we survey the contributions of Grassmann to our terminology.

It's well known that Grassmann introduced the notions of inner and outer products; less known perhaps is the history of the term ``outer product''.

\todo{put text about the two kinds of outer product here}


Grassmann's \textit{Ausdehnungslehre} of 1862\cite[Ch. 2]{Grassmann1862} introduces describes general structures to form products of two or more vectors. In modern language, if two vectors are expressed in some basis

\begin{align}
u & = \sum_i c_i e_i \\
v & = \sum_i d_i e_i \\
\end{align}

then any vector product operation $\circ$ is fully defined if the result of the operation $\circ$ on the basis vectors $e_i$ is known, since

\[
u \circ v = \sum_{ij} c_i d_i (e_i \circ e_j) = \sum_{ij} c_i d_i G_{ij}
\]

where we introduce formally $G_{ij}$, the Gramian of the vector space with basis ${e_i}$ and inner product $\circ$.

Grassmann defined the inner product to be the product whose Gramian is the identity, and the outer product by placing new basis vectors in each uniquely defined entry of an antisymmetric $G$. Interestingly, Grassmann does not give the general case a name, instead referring to it informally in the text defining his notation:

,,Ein Produkt, in welchern die Faktoren $a, b, \cdots$ irgend wie enthalten find, werde ich[...] mit $P_{a,b,...}$ bezeichnen`` \cite[p. 24, \S 43]{Grassmann1862}

``A product in which the factors a, b, ... are included in any way I will[...] denote by $P_{a,b,...}$''~\cite[p. 22, \S 43]{Grassmann2000}

Gibbs, however, recognized the value of the general case, bequeathing it the name ``indeterminate product''.~\footnote{Some of the English literature incorrectly attribute the name to Grassmann; however, it is quite clear from \textit{Multiple Algebra} that the name is his invention. The closest phrase used by Grassmann is ,,ein beliebiges Produkt $P_{a_1, a_2, ..., a_n}$`` (``an arbitrary product''~\cite[p. 196, \S 353]{Grassmann2000}), which is not defined in a formal, technical sense.} If we interpret this as using an unsymmetric Gramian $G$ and placing a new unique basis vector in each entry of $G$, then we have the basic ingredients of a tensor product.

Grassmann's other contribution to this topic is the concept of ,,offne Produkt`` or ``open product''~\footnote{Somewhat confusingly, Grassmann's 1862 book defines open products, or ``product[s] with n \{interchangeable\} openings''~\cite[\S\S 353, p. 196]{Grassmann2000}, in a way which we would recognize today as symmetric tensors of rank $n$.}, which he writes in Ausdehnungslehre 1844, Sec 172, p 267 (English pp 271-2) with the notation $[A() . B]$, acting on a vector $P$ by

\[
[A() . B] P = AP . B,
\]

or in modern notation,

\[
(b a^T) p = (a\cdot p) b,
\]

transcribing vectors into lower case letters in line with Householder's
convention. In other words, Grassmann's open product is what we would call
today the outer product, where ``open'' refers to the presence of the empty
parenthesis denoting an ``opening''. Gibbs recognized the open product as a
matrix in Multiple Algebra, Collected Works vol 2, p 94, but it is specifically
a rank 1 matrix. \todo{Possibly the notion of rank did not exist then...?}

In Gibbs's lecture notes of 1884, Sec. 107, p. 53 of Collected Works v2, he
names the result of the indeterminate product of two vectors (in three
dimensions) a dyad, and the linear combination of three dyads a dyadic, or what
we would call today a $3\times3$ matrix. c.f.\ the published version in
Wilson1901.

Vectors were called ``extensive magnitudes''. Relative vectors were called
``displacements'' (,,Strecke``) (\S 14) and were distinguished from absolute
vectors, being coordinates of points.

Systematic convention: Capital Roman letters were names of points (absolute
vectors). Small Roman letters were names of displacements (relative vectors).

Ausdehnungslehre I introduced the ``open product'' (,,offne Produkt``) (\S 172) as the quantity
\[
S = [A_1().B_1 + A_2().B_2 + \dots]
\]
%
or in modern notation, what we recognize today as the expansion of a matrix $S$ in rank-1 outer products
\[
S = \sum_i b_i a_i^T.
\]

A single term, written $[A().B]$ in Grassmann's notation, is a ,,Produkt mit
ein Lücke`` (product with one opening), which is generalized to multiple
openings in Ausdehnungslehre II. Such a single term is what we would today call
an outer product of two vectors, or a rank 1 matrix.

Ausdehnungslehre I introduces the concept of outer multiplication (,,äussere
Multiplikation``)~\cite[\S 34, p.57]{Grassmann1844}\cite[p. 81]{Grassmann1995}
as what we would today call the exterior product or wedge product. The term
,,äusseres Produkt``\cite[\S 36, p. 60]{Grassmann1844} (``outer
product''\cite[p. 84]{Grassmann1995}) also appears.

\paragraph{Grassmann (1847)~\cite{Grassmann1847,Grassmann1995}}

\cite[p. 16]{Grassmann1847}\cite[\S 7, p. 334]{Grassmann1995} introduces the
 ,,innere Produkt`` inner product $a \times b$, and the
 ,,innere Quadrat`` inner square $a^2 = a \times a$.



\paragraph{Hamilton (1847)~\cite{Hamilton1847}}

Introduces the term ``scalar product'' as part of the quaternion product.
\S. 12 introduces the term ``scalar product'' for the case of parallel vectors (which differs from modern usage by a negative sign).
He does not appear to have introduced the general case of non-parallel vectors.



\paragraph{Cauchy (1847)~\cite{Cauchy1847}}

Credited \todo{by whom?} as the earliest description of $n$-dimensional space.

\begin{quote}
Concevons maintenant que le nombre des variables $x, y, z, \dots$
devienne sup\'erieur \`a trois. Alors chaque syst\`eme des valeurs de
$x, y, z, \dots$ d\'eterminera ce que nous appellerons un \text{point analytique}, dont
ces variables seront les \textit{coordonn\'ees} et, \`a ce point, r\'epondra une
certaine valeur de chaque fonction de $x, y, z \dots$

Nous appellerons encore \textit{droite analytique} un syst\`eme de \textit{points
analytiques} dont les diverses coodonn\'ees e'exprimeront \`a l'aide de
fontions lin\'eares donn\'ees de l'une d'entre elles.
\end{quote}



\paragraph{Sylvester (1850)~\cite{Sylvester1850}}

Introduces the term matrix~\cite[p. 150]{Sylvester1850}:

\begin{quote}
For this purpose we must commence, not with a square, but with an oblong arrangement
of terms consisting, suppose,
of $m$ lines and $n$ columns. This will not in itself represent a determinant,
but is, as it were, a Matrix out of which we may form various systems of
determinants by fixing upon a number $p$, and selecting at will $p$ lines and $p$
columns, the squares corresponding to which may be termed determinants
of the $p$th order.
\end{quote}



\paragraph{Sylvester (1851)~\cite{Sylvester1851}}

Justifies the term ``matrix'' with its older vernacular meaning:

\begin{quote}
I have in previous papers defined a ``Matrix'' as a rectangular array of
terms, out of which different systems of determinants may be engendered,
as from the womb of a common parent[...] - p. 247
\end{quote}



\paragraph{Hermite (1854)~\cite{Hermite1854}}

In a paper studying quadratic forms in three variables, introduced the notation
\[
    f = ax^2 + a'y^2 + a''z^2 + 2byz + 2b'xz + 2b''xy
      = \begin{pmatrix}a,a',a''\\b,b',b''\end{pmatrix}
\]
which we might write today
\[
    f = \begin{pmatrix}x\\y\\z\end{pmatrix}^\prime
        \begin{pmatrix}a&b''&b'\\b''&a'&b\\b'&b&a''\end{pmatrix}
        \begin{pmatrix}x\\y\\z\end{pmatrix}
\]

\paragraph{Cayley (1855)~\cite{Cayley1855}}

Studied quadratic functions using the notation of matrices.

The atrix he writes in the double bar form

\[\begin{vmatrix}
\alpha,& \beta,& \gamma,& \dots \\
\alpha',& \beta',& \gamma',& \dots \\
\alpha'',& \beta'',& \gamma'',& \dots \\
\vdots
\end{vmatrix}\]

sometimes with parentheses enclosing the first row only.

He invited a notation for matrix--vector products

\begin{verbatim}
(| a, h, g, ... |)(x, y, z)
| h, b, f, ... |
| g, f, c, ... |
| vdots        |
\end{verbatim}
with the intelocking parenthesis.

The components of the result he write as $(\alpha, \beta, \gamma, \dots)(a, a', a'')
= \alpha a + \beta a' + \gamma a'' + ...$ which is clearly the scalar product,
but he does not name them.

Here are quadratic forms

\begin{verbatim}
(| a, h, g, ... |)(x, y, z)^2
| h, b, f, ... |
| g, f, c, ... |
| vdots        |
\end{verbatim}

with the intelocking parenthesis
for $ax^2 + by^2 + cz^2 + 2fyz + 2gzx + 2hxy + \dots$.

Similarly, he writes for the bilinear form (le fonction lineo-linéaire)

\begin{verbatim}
(| a, h, g, ... |)(\xi, \eta, \zeta)(x, y, z)^2
| h, b, f, ... |
| g, f, c, ... |
| vdots        |
\end{verbatim}

The matrix part he later denotes with a diamond $\Diamond$.

He also derived the rule for matrix multiplication, which he described as the
the composition of matrices.
\begin{quote}
Il faut faire attention, dans la composition des matrices, de combiner les \textit{lignes}
de la matrice à gauche avec les \textit{colonnes} de la matrice à droite, pour former les
\textit{lignes} de la matrice composée.
- p. 187

Care must be taken in the composition of matrices to combine [rows]
of the matrix on the left with the columns of the matrix on the right to form the
[rows] of the composite matrix.
\end{quote}



\paragraph{Cayley (1858)~\cite{Cayley1858}}

Introduced the term ``transpose of a matrix''.

Cayley writes the row-column product as

\begin{verbatim}
(a, b, c) tr. ( a', b', c' ) = (a, b, c)( a' ) = (a, b, c)(a', b', c')
|       |     |            |   |       || b' |
                                        | c' |
\end{verbatim}

\paragraph{Cayley (1858)~\cite{Cayley1858b}}

Studies ``bipartite quadric functions''/``lineo-linear functions'', or what we
call today bilinear forms.

\begin{verbatim}
( a, b, c )(x, y, z)(x-roman, y-roman, z-roman)
|a', b', c'|
|a'', b'', c''|
\end{verbatim}

The matrix part he later denotes with $\Omega$.

and ``quadric functions''

\begin{verbatim}
(| a, h, g |)(x, y, z)^2
| h, b, f |
| g, f, c |
\end{verbatim}

for $ax^2 + by^2 + cz^2 + 2fyz + 2gzx + 2hxy$.

p. 479 expains the composition rule for multiplication

\begin{quote}
    It is to be observed, that the operation [of multiplication or composition of
    two matrices] is not a commutative one; the component matrices may be
    distinguished as the fist or ofurther compoentn matrix, and the second or
    nearer component matrix, and the rule of composition is as follows, viz.\ any
    \textit{line} of the compoounted matrix is obtained by combing the corresponding
    \textit{line} of the first or further component matrix successively with the several
    \textit{columns} of the second or neare compound matrix.

    [We may conveniently write

    \begin{tabular}{r|rrr}
    \multicolumn{1}{r}{} & $(\alpha,\alpha',\alpha'')$, & $(\beta,\beta',\beta'')$, & $(\gamma,\gamma',\gamma'')$,\tabularnewline
    \cline{2-4}
    $(a,b,c)$ & $(a,b,c)\!\!(\alpha,\alpha',\alpha'')$ & $(a,b,c)\!\!(\beta,\beta',\beta'')$ & $(a,b,c)\!\!(\gamma,\gamma',\gamma'')$\tabularnewline
    $(a',b',c')$ & $(a',b',c')\!\!(\alpha,\alpha',\alpha'')$ & $(a',b',c')\!\!(\beta,\beta',\beta'')$ & $(a',b',c')\!\!(\gamma,\gamma',\gamma'')$\tabularnewline
    $(a'',b'',c'')$ & $(a'',b'',c'')\!\!(\alpha,\alpha',\alpha'')$ & $(a'',b'',c'')\!\!(\beta,\beta',\beta'')$ & $(a'',b'',c'')\!\!(\gamma,\gamma',\gamma'')$\tabularnewline
    \end{tabular}

    to denote the [product].]
\end{quote}


\paragraph{Grassmann (1862)~\cite{Grassmann1862,Grassmann2000} - Ausdehnungslehre II.}

Writes inner product as $[A | B]$, mentions $[A \times B]$ as an alternative notation (\S 137, p93 English).

The outer (tensor) product is introduced without a name (Remark after \S 51)

\begin{quote}
,,Ein Produkt, in welchern die Faktoren $a, b, \cdots$ irgend wie enthalten find, werde ich[...] mit $P_{a,b,...}$ bezeichnen`` \cite[p. 24, \S 43]{Grassmann1862}

``A product in which the factors a, b, ... are included in any way I will[...] denote by $P_{a,b,...}$''~\cite[p. 22, \S 43]{Grassmann2000}
\end{quote}

Later, the same quantity is referred to informally as
,,ein beliebiges Produkt $P_{a_1, a_2, ..., a_n}$``~\cite[\S 353]{Grassmann1862} (``an arbitrary product''~\cite[p. 196, \S 353]{Grassmann2000})

Also introduced the combinatorial product (Ch. 3), which we recognize today as the determinant.


\paragraph{Laguerre (1867)~\cite{Laguerre1867}}

Seemingly independently of the works of Cayley and Grassmann, Laguerre wrote down
le calcul des systèmes linéaires in a letter to Hermite. He wrote that he would
represent ``linear systems'' by single capital letters, being quantities imbued
with their own algebraic operations:

\begin{quote}
    J'appelle, suivant l'usage habituel, \textit{système linéaire} le tableau
    des coefficient d'un système de $n$ équations linéaires à $n$ inconnues.
    Un tel système sera dit \textit{système linéaire d'order} $n$ et,
    sauf une exception dont je parlerai plus loin, je le représentarai
    oujour par une seule letter majuscule, réservant les lettres
    minuscules pour désigner spécialement les éléments du système linéaire.

    Ainsi, par exemple, le système linéaire
    \[
        \begin{matrix}\alpha & \beta \\ \gamma & \delta \end{matrix}
    \]
    sera représenté par le seule lettre majuscule $\bf A$. Dan tout ce qui
    suit, je considérerai ces lettres majuscules représentant les systèmes
    linéaires comme de véritable quantités, soumises à toutes
    les opérations algébriques.
\end{quote}

He then proceeds to write down the laws for matrix addition, subtraction,
multiplication $\bf C = \bf A \bf B$,
diagonal matrices (systèmes simples), transposed matrices (systèmes inverses)
$\bf A_1$, symmetric matrices (systèmes symetriques), skew-symmetric matrices
(systèmes gauches),and the adjugate matrix (systèmes reciproques) $\bf A_0$
(up to a scaling factor).

Laguerre writes down the equivalent of row and column matrices in a zero-padded form,
\[
\bf X = \begin{pmatrix}x_1&x_2&\ldots&x_n\end{pmatrix} = \begin{matrix}
x_1&x_2&\ldots&x_n\\
0&0&\ldots&0\\
0&0&\ldots&0\\
\ldots&\ldots&\ldots&\ldots
\end{matrix}
\]

Laguerre then writes the quadratic form using this pure matrix notation as follows

\begin{quote}
En sorte que si, par exemple, on a
\[
f = ax^2 + 2bxy + cy^2,
\]
cette relation, qui est équivalente à
\[
    \begin{matrix}f&0\\0&0\end{matrix} =
    \begin{matrix}x&y\\0&0\end{matrix} \times
    \begin{matrix}a&b\\b&c\end{matrix} \times
    \begin{matrix}x&0\\y&0\end{matrix},
\]
pourr s'écrire
\[
    \Omega f = {(x, y)}_1 \bf A (x, y)
\]
en posant $\begin{matrix}a&b\\b&c\end{matrix}=\bf A$,
ou encore
\[
    \Omega f = \bf X_1 \bf A \bf X
\]
on posant $(x, y) = \bf X$.
\end{quote}
where $\Omega$ was previously defined as $\begin{matrix}1&0\\0&0\end{matrix}$.



\paragraph{B. Peirce (1873)~\cite{Peirce1873} and C. S. Peirce (1874)~\cite{Peirce1874}}

Developed the notion of matrix unit, which he called ``elementary relative'' in \cite[p.359]{Peirce1873}.

B. Peirce (1874)~\cite{Peirce1874} explains that these units, which he called ``vids'', form the basis of a linear algebra.
This paper also appears to be the earliest use of the phrase ``linear algebra'' (at least, in English).

In 1883~\cite{Peirce1883} C. S. recognizes the significance for the algebra of matrices.

\paragraph{Frobenius (1878)~\cite{Frobenius1878}}

May have invented the term ``bilinear form''.
The product is similar to the product law of two determinants (matrices?) (p. 346)


\paragraph{Clifford (1878)~\cite{Clifford1878}}

Vectors, a.k.a.\ carriers, first, ``from their analogy to a step of translation or carrying''.
Denoted with small Greek letters, p. 13.
Matrices are linear transformations between vectors. Uses Cayley's notation for matrices, p. 164.
Notable for introducing vector and scalar products as separate.
Vector products are $V\alpha\beta$ and scalar products are $A\alpha\$beta$, p. 95.



\paragraph{Cayley (1880)~\cite{Cayley1880}}

Cayley continues to build on the notation of his 1858 Memoir~\cite{Cayley1858}.
He now writes $(a)$ for the transpose of a matrix, which he denotes by
``square-letters'' $a$. Vectors show up as ``row-letters'' $u$.
The general form he now writes as $(*)(x, y)^2$.

\paragraph{Gibbs (1881)~\cite{Gibbs1881}}

Unpublished lecture notes on vectors (see \cite{Wilson1901} for published version).

Introduced notation of small Roman letter in boldfact (``Clarendon type'') for vectors.

The dot product is $\mathbf a \cdot \mathbf b$.
\cite[p. 55]{Wilson1901} introduces the direct product, ``read \textbf{A} \textit{dot} \textbf{B} and therefore may be called the dot product instead of the direct product. It is also called the scalar product owing to the fact that its value is scalar.''.

He acknowledges Grassmann's \textit{Ausdehnungslehre} as containing important concepts that were unnamed. Gibbs decides to call the general product case an ``indeterminate product''.

Gibbs discusses vectors in the case of $n=3$ dimensions only. In this setting the outer product is called a dyad, written with the variables juxtaposed, $\mathbf{ a \; b }$.
Gibbs stresses to think of this as a formal product waiting to act upon a vector

The dyadic or dyadic product is $\mathbf{ a \; b } + \mathbf{ c \; d } + \mathbf{ e \; f }$ and is what we would recognize today as the rank 1 expansion of a $3 \times 3$ matrix in outer products.

\todo{Did Gibbs do any of this in the primary literature before this work?}

\todo{Gibbs 1886 is also worth discussion separately}


\paragraph{Weierstrass (1884)~\cite{Weierstrass1884}}

Publishes the notion of defining formal linear algebra through its structure constants. (He does not give the numbers a name.)



\paragraph{Dedekind (1885)~\cite{Dedekind1885}}

Extends Weierstrass to give the restriction on the structure constants so that the resulting algebra is associative. (He does not give the numbers a name either.)



\paragraph{Heaviside (1886)~\cite{Heaviside1886}}

Of significance for systematically introducing boldface small Roman letters (in ``Clarendon type'') to denote vector variables.~\cite{Heaviside1886}. (Heaviside gives a very amusing diatribe against the prevailing custom of Greek latters by Tait and Germanic Gothic letters by Maxwell in \cite[\S 103, pp. 139-142]{Heaviside1894})

\todo{did he cite Gibbs?}

\cite[\S 107]{Heaviside1894} defines the scalar product as
\[
\mathbf{AB} = \mathrm{AB} \cos \theta
\]
and calls A and B the ``tensors'' of $\mathbf{A}$ and $\mathbf{B}$. The cross product $\mathbf a \times \mathbf b$ he writes as $\mathbf{V a b}$.

\paragraph{Peano (1888)~\cite{Peano1888}}

Formalized axioms of vector space latent in Grassmann's work. Note though that \cite{Moore1995} argues that the axiomatization effort was not influential in the development of mathematics until much later.

May have also reintroduced the scalar product?

Peano used [Peano 1887]
for the scalar product, but called it “prodotto di due segmenti” (since in this book vectors
were called ‘segmenti’). He used
again later [Peano 1891a] and called it “prodotto
(interno o geometrico).”



\paragraph{Scheffers (1889)~\cite{Scheffers1889}}

Formal linear algebra. Did he copy Weierstrass?? His paper has the same name.


\paragraph{Taber (1890)~\cite{Taber1890}}

A historical overview of the development of the theory of matrices.
Recognized that matrices were latent in the theory of vectors as developed by Hamilton in his \textit{Lectures on Quaternions}\~cite{Hamilton1852}.
Taber has a sense of outer product and its role in defining matrices (``linear unit operator[s]'') via bilinear forms. In modern language,
a matrix $\rho$ is written in the rank-1 expansion form:
\[
\rho = x_1 \alpha_1 + x_2 \alpha_2 + \dots + x_\omega \alpha_\omega
\]
then the linear unit operator $\phi$ can be expressed as a matrix by considering its action on each basis vector $\alpha$
\[
\phi\rho = x_1.\phi\alpha_1 + x_2.\phi\alpha_2 + \dots + x_\omega.\phi\alpha
\]
which can be used to develop the system of equations
\begin{align}
\phi \alpha_1 & = a_{11} \alpha_1 + a_{21} \alpha_2 + \dots + a_{\omega 1} \alpha_\omega \\
\phi \alpha_2 & = a_{12} \alpha_1 + a_{22} \alpha_2 + \dots + a_{\omega 2} \alpha_\omega \\
              & \cdots \cdots \cdots \\
\phi \alpha_\omega & = a_{1\omega} \alpha_1 + a_{2\omega} \alpha_2 + \dots + a_{\omega \omega} \alpha_\omega \\
\end{align}
thus turning the definition of $\phi$ into a system of linear equations over
the basis vectors $\alpha$, when then defines the linear transformation $\phi$
using the matrix of coefficients ${(a)}_{ij}$ in the system of equations.  (Note
that Tabor's indexing is row major, in contrast with modern convention.)
Intriguingly, Tabor also uses a very suggestive notation
\[
\phi\rho = (\phi|x_1)\alpha_1 + (\phi|x_2)\alpha_2 + \dots + (\phi|x_\omega)\alpha
\]
Furthermore, Taber reviews C S Peirce's ``forms'' or ``vids'' $(\alpha_i :
\alpha_j)$, and says they are equivalent to ``elementary units'' of a matrix.
It is clear that Peirce's notation corresponds to $a_j a_i^T$ today.  He
introduces Clifford's ``quadrates'' (which are just matrices).
\begin{quote}
I shall therefore call an algebra linear in $\omega^2$ of these vids a quadrate
algebra of order $\omega$; and any expression linear in the vids, a
\textit{quadrate form}.
\end{quote}
Thus introducing the term that is almost the modern form, a quadratic form.

\paragraph{Molien (1892)~\cite{Molien1892}}

Presents the first systematic treatment of the linear algebra of hypercomplex numbers.
In modern terms, we would say that the algebra is characterized by structure constants $c_{ij}^k$ such that each basis element of the algebra satisfies the multiplicative identities
\[
e_i e_j = \sum_k c^k_{ij} e_k
\]
Molien presents the inverse relation.
\begin{quote}
,,Das distributive Gesetz der Multiplication verlangt, dass das Product
zweier complexer Zahlen eine Zahl sei, deren Parameter sieh als bilineare
Formen darstellen, die mit Hilfe constanter Grössen aus den beiden
Reihen der Parameter der Factoren gebildet sind.``
\end{quote}
For $u, x, x^\prime$ in the hypercomplex number system, the multiplication law
requires
\[
x_i^\prime = \sum_{kl} a^i_{kl} x_k u_l
\]
Molien also uses the word ,,Basis des Zahlensystems`` (basis of the number system).

An important secondary source for the significance of Molien's work is~\cite{Hawkins1972}.

Gibbs and Heaviside (????) - may have introduced the scalar product?

Molien \url{http://link.springer.com/article/10.1007/BF01443450}

Hawkins \url{http://link.springer.com/article/10.1007/BF00328434}

\paragraph{Weber (1895)~\cite{Weber1895}}

A very old school book about algebra.

Matrices show up in the original form [p. 82] as arrays out of which determinants are formed.

They are not given variable names..??

\paragraph{Muth (1899)~\cite{Muth1899}}

Matrices don't exist yet. Everything is discussed in terms of bilinear forms

\[
A = \sum a_{ik} x_i y_k
\]

and their determinants.


\paragraph{Ricci and Levi-Civita (1900)~\cite{Ricci1900,Hermann1975}}

The famous paper on differential geometry and tensor analysis.

They introduce the tensor product, but simply call it the
«produit des deux systèmes [covariants ou contrevariants]»~\cite[p. 133]{Ricci1900}, translated as
``the (tensor) product of the two (covariant [or contravariant]) tensor fields''~\cite[p. 28]{Hermann1975}.

p. 135 introduces the quadratic form of two 3-dimensional differential elements as a «forme fondamentale $\varphi$[...] en coordonnées générales» (``fundamental form $\varphi$[...] in general coordinates)

\[
\varphi = \sum_1^3 \text{\tiny{rs}} a_{rs} dx_r dx_s
\]

in other words, they spell out the quadratic form in terms of the elements.



\paragraph{Prandtl (1903) and the German Vector commission}

Recommended the notation of $\mathbf{a} \cdot \mathbf{b}$ a la Gibbs but called it the inner product à la Grassmann.



\paragraph{Burali-Forti and Marcolongo (1907-1908) - Per l’unificazione delle notazioni vettoriali.}

Proposed $\mathbf{a}\times\mathbf{b}$ for the scalar product and  $\mathbf{a}\wedge\mathbf{b}$ for the vector product.

\url{http://link.springer.com/article/10.1007/BF03013528}
\url{http://link.springer.com.libproxy.mit.edu/article/10.1007/BF03015054}
\url{http://link.springer.com.libproxy.mit.edu/article/10.1007/BF03015068}
\url{http://link.springer.com.libproxy.mit.edu/article/10.1007/BF03029137}
\url{http://link.springer.com.libproxy.mit.edu/article/10.1007/BF03013528}


\paragraph{Hahn (1911)~\cite{Hahn1911}}

Possibly the earliest mention of the word ,,eigenvektor`` (``eigenvector'') \cite[p. 35]{Hahn1911}.
He introduces eigenvectors as representations of integrated eigenfunctions corresponding to a finite-dimensional integral kernel.



\paragraph{Cullis (1913)}

First monograph on matrices that pays attention to the properties of
rectangular matrices, not just square ones. $A$ and $x$ are their own objects,
not just tables of numbers.
Cullis is notable for introducing the modern typographic conventions for types
variables. In particular matrices are capital Roman, vectors (as ``horizontal
rows'' or ``vertical rows'' of a matrix) are small Roman, and scalars as small
Greek appear here.  Another notable notational convention is the use of $A =
{[a]}^m_n$ and the transpose as $A' = \overbracket{\underbracket{a}}^m_n$. The
prime in $A'$ is used informally in the sense of ``a quantity derived from A'',
but often (if not always) is used to denote the transpose.
The bracket convention for the transpose is worth explaining further in how it
works to fill in the symbolic entries...

Another notational convention that is noteworthy is Cullis's use of what we
would today call the trailing singleton dimension rule. For column and row
matrices, the elements of those matrices are not denoted by $a_{1n}$ or
$a_{m1}$, but rather just $a_n$ or $a_m$. In other words, he drops the 1 when
convenient. He also does this to the brackets: $x = {[x]}_n$ is equivalent to
${[x]}^1$.
Cullis 1928 introduces bilinear forms in notation that is almost recognizable to us today...



\paragraph{Einstein and Grossmann (1913)}

A pamphlet on general relativity, divided into two parts for the physical and mathematical content. In Part II (by Grossmann) we see

\begin{quote}

,Die der gewöhnlichen Vektoranalysis entnommenen Bezeichnungen ,,äußeres und inneres Produkt`` rechtfetigen sich, weil jene Operationen sich letzten Endes als besondere Fälle der hier betrachteten ergenen.`~\cite[p. 26]{Einstein1913}\cite[p. 327]{Einstein1995}

`The designations ``inner and outer product'', which are taken from ordinary vector analysis, are justified because, when all is said and done, those operations prove to be special cases of the operations considered here.'~\cite[p. 175]{Einstein1996}
\end{quote}
The name ``outer product'', however, appears to be original. This pamphlet references only \cite{Ricci1900}, but that paper only contains the term ``produit''.
The borrowing of the term ``outer product'' may be responsible for the conflation of the two meanings in present usage, as later English books about relativity consistently use one meaning or the other. either meaning. For example,

\cite[p. 25]{Murnaghan1922} uses ``outer product'' in Grassmann's sense, ``direct product'' for tensor product, and ``inner product'':
\begin{quote}
``The difference $X_{rs} - \overline X_{rs}$ [...] should be more important than either of the direct products $X_{rs}$ [$\equiv X_r \cdot \overline X_s = X \cdot \overline X$] or $\overline X_{rs}$. It is what Grassmann called the \textit{outer product} of the two tensors $X, \overline X$.''
\end{quote}

\cite[p. 87]{Carmichael1920} uses ``outer product'' for the tensor product and the ``inner product'' in line with Einstein and Grossmann:
\begin{quote}
``[W]e used the term product of two tensors to denote the tensor whose component elements are all the elements formed by multiplying an element of one tensor by the element of another tensor. This may be called their \textit{outer product}. We also need the notion of \textit{inner product} of two vectors, say of $A_\mu$ and $B^\mu$; and this is defined to be the quantity $\sum_\mu A_\mu B^\mu$; that is, the sum of products of corresponding elements.''
\end{quote}
(These terms are absent in his first edition of 1913)

\cite{Silberstein1922} is interesting for the mention of the ``outer product of two vectors'', and also discussing the inner product as being derived from the contraction of the outer product.
\begin{quote}
(\S 15, p. 43) Consider, on the other hand, what is known as \textit{the outer product} of two vectors, of the same or of opposite kinds, \textit{i.e.}, $A_\iota B_\kappa$, or $A^\iota B^\kappa$, or $A_\iota B^\kappa$.

(\S 18, p. 48) \textit{The inner multiplication}, already meThe outer multiplication combined with contraction (when there are indices to contract) gives the inner product. Thus the inner product of $A_\iota$ and $B^\kappa$ is
\[
A_\kappa B^\kappa = M^\kappa_\kappa = M,
\]
an invariant. (There is no inner product of $A_\iota$, $B_\kappa$.)
\end{quote}



\paragraph{Dickson (1914)~\cite{Dickson1914}}

One of the earliest books on ``linear algebras''.
Explains that ``matric algebra'' is a linear associative algebra.

Cited Molien, Cartan and Wedderburn as influential.


\paragraph{Courant (1920)~\cite{Courant1920}}

Next earliest use of the word ,,eigenvektor`` (after Hahn, 1911).
In, \S 10 he specifically talks about ``vectorial eigenvalue problems''. He
writes

\begin{quote}
,,Man kann ferner analoge Eigenwertprobleme untersuchen, bei denen an Stelle
der gesuchten Funktion $u$ ein Vektor $u$ steht, welcher ebenfalls am Rande
gewissen homogenen Randbedingungen unterworfen ist.``

``One can further examine analogous eigenvalue problems, where instead of the
unknown function $u$ a vector $u$ stands in, which is also subject to certain
homogeneous boundary conditions on the edge.''
\end{quote}

Clearly the implication here is that eigenvalue problems have been
traditionally about eigenfunctions, not eigenvectors.



\paragraph{Eisenhart (1922)}

\url{https://archive.org/stream/continuousgroups032317mbp#page/n153/mode/2up/search/vector}

\paragraph{Courant and Hilbert (1924~\cite{Courant1924}}

The monumental work on mathematical physics. Apparently the first systematic treatment of matrix algebra for the physics audience?



\paragraph{Dickson (1924)~\cite{Dickson1924}}

An early book on modern algebra that is matrices only.

Wedderburn is the main influence on this book.

\S 9, p 17 defines the ``\textit{constants of multiplication} of the algebra $A$''
which are what we call today structure constants.



\paragraph{Born and Jordan (1925)~\cite{Born1925}}

Explains quantum mechanics as matrix mechanics. First major work to introduce matrices into the physics literature.

They are familiar with the notions of vector analysis, but mainly in 3 and 4 dimensions.

,,skalare Produkt`` $(\mathfrak A, \mathfrak B)$ (p. 885)



\paragraph{Dickson (1926)~\cite{Dickson1926}}

An early book on modern algebra that is matrices only.


Ch. 3, p. 39 -
\begin{quote}
[A] rectangular table composed of $mn$ numbers $a_{ij}$ arranged in
$m$ rows with $n$ numbers in each row is called a \textit{matrix} with $m$ rows
and $n$ columns, or briefly an $m\times n$ matrix. We shall speak of $A$ as the
matrix of (the coefficients of) the linear forms[.]

Examples of one-rowed matrices are the notations $(x, y)$ and
$(x, y, z)$ for points in a plane and in space, the elements of these
matrices being the coordinates of the points.
\end{quote}

Transpose is $C'$

No Matvecs. No vectors.


\paragraph{Schreier and Sperner (1931)~\cite{Schreier1931}}

Vectors come first. They have a very careful treatment building from coordinates
to vectors and finally matrices.

Schreier and Sperner begin with the concept of $n$-dimensional affine space
as a collection of $n$-tuples: p. 9

\begin{quote}
    Ein geordnetes $n$-Tupel reeller Zahlen, d.\ i.\ ein System $(x_1, x_2, \dots, x_n)$
    von $n$ reellen Zahlen in bestimmter Reigenfolge, heißt \textit{Punkt} [...]
    Die Zahlen $x_1, x_2, \dots, x_n$ heißen \textit{Koordinaten} des Punktes [...]
    Die Gesamtheit aller $n$-Tupel reeler Zahlen heißt \textbf{der affine
    Raum von $n$ Dimensionen}.

    An ordered $n$-tuple of real numbers, i.\ e.\ a system $(x_1, x_2, \dots, x_n)$ of
    $n$ real numbers in a specific order, is called a \textit{point} [...]
    The numbers $ x_1, x_2, \ dots, x_n$ are called \textit{coordinates} of the point [...]
    The set of all $n$-tuples of real numbers is called \textbf{the affine
    space of $n$ dimensions}.
\end{quote}

They then introduce vectors ,,von der Anschauung ans`` (``intuitively'') through a discussion of
the coordinate geometry of three-dimensional Euclidean space, before introducing
vectors as ordered line segments between points in an affine space. Next, they
introduce the notion of linear dependence of vectors, then describe ``lineare
Raume'' (``linear spaces'') that are essentially our modern linear vector spaces,
and finally systems of linear equations and their encodings in matrix form as
,,[ein] rechteckiges Schema von Zahlen oder Elementen`` (``a rectangular scheme
of numbers or elements'', p. 37).
Only then do they introduce the fact that the columns of a matrix are then vectors
in their own right because they obey the peropeties of a linear space.

\begin{quote}
    Einer jeden Spalte der Matrix ordnen wir jetzt einen Vektor zu, und zwar der
    $k$-ten Spalte den Vektor der die Elemente der $k$-ten Spalte der Reihe nach
    zu Komponenten hat. Diese \textit{Spaltenvektoren} $\mathfrak a_k$ sind
    $m$-dimensionale Vektoren.

    To each column of the matrix we assign a vector, namely to the $k$-th column
    the vector with the components of the $k$-th column sequentially.
    These \textit{column vectors} $\mathfrak a_k$ are $m$-dimensional vectors.

    p. 37
\end{quote}


Interestingly, matrix multiplication does not show up until much later in the
discussion of the algebraic properties of linear transformations. After being
encoded into matrices, they then introduce the product of two (square) matrices.

They write, II. \S 5 p.50,

\begin{quote}
Das Element in der $i$-ten Zeile und $k$-ten Spalte der Productmatrix ist das
Skalarprodukt des $i$-ten Zeilenvektor $\{a_{i1}, a_{i2}, \dots, a_{in}\}$
von $A$ met dem $k$-ten Spaltenvektor $\{b_{1k}, b_{2k}, \dots, b_{nk}\}$ von $B$.

Das erinnert an die Multiplikation von Determinanten.

The element in the $i$-th row and $k$-th column of the product matrix is the
scalar product of the $i$-th row vector $\{a_{i1}, a_{i2}, \dots, a_{in}\}$ of $A$
and the $k$-th column vector $\{b_{1k}, b_{2k}, \dots, b_{nk}\}$ of $B$.
\end{quote}



\paragraph{Van der Waerden (1930/1)~\cite{vdWaerden1930,vdWaerden1931}}

Vectors came first!


Credited in \cite{Kleiner2007} as the origin of the term ``Linear algebra''. But
see Peirce (1874).

\cite[p. 36]{vdWaerden1975} says that `The contents of Chapter 15 (``Lineare Algebra'') were generally
known in 1924.'


\cite[\S 10, pp. 43-44]{vdWaerden1930} has some very interesting things to say
about vectors as ``rows'' of $n$ numbers, their relation to hypercomplex numbers,
and structure constants relating ,,Basiselementes``:

\begin{quote}
und bilde alle möglichen Reihen (...) von n
mit Nummern versehenen Elementen von K.


\end{quote}

\cite[\S 104, pp. 110. ff.]{vdWaerden1931} has more interesting things.
Especially about baby matmul.
One writes a linear form $\sum u_i \lambda_i$ usefully as a product of a row (u...) with a column (lambda...), which the vector (lambda...) represents.



\paragraph{Weyl (1928)~\cite{Weyl1928,Weyl1931}}

The book begins with a chapter on $n$-dimensional vector spaces.
Vectors come first.

Introduces a vector $\mathfrak x$ as ``a set of $n$ ordered numbers $(x_1, x_2 \cdots, x_n)$.

\begin{quote}
,,So beginne ich mit der Erklärung, daß ein \textit{Vektor} $\mathfrak x$ des $n$-dimensionalen linearen Raumes
$\mathfrak R = \mathfrak R_n$
ein Satz von $n$ in bestimmter Reihenfolge gegebenen Zahlen $(x_1, x_2, \dots, x_n)$
ist. Die Vektorrechnung erscheint als ein Rechnen mit solchen mehrgliedrigen
Zahlreigen.`` - \cite[p. 4]{Weyl1928}

``I begins with the explanation that a \textit{vector} $\mathfrak x$ in the $n$-dimensional
linear space $\mathfrak R = \mathfrak R_n$ is a set of ordered numbers $(x_1, x_2, \cdots, x_n)$;
vector analysis is the calculus of such ordered sets.`` - \cite[p. 1]{Weyl1931}
\end{quote}

Matrices are introduced in the next section in the context of ,,Lineare Abbildungen``
(``linear maps'').

Weyl explains that a $p \times n$ matrix $A$ is a mapping from a the original
$n$-dimensional vector space $\mathfrak R$ to another $p$-dimensional vector space
$\mathfrak S$. However, he explains that to compute the action of $A$ on $x$, one
must first place the entries of $x$ and $y$ into column matrices, then use the
rules of matrix multiplication to compute the product:

\begin{quote}
,,Der Matrizenkalkül stellt uns auch für die Abbildungsformeln wie [die Gleichungen]
 eine abgekürzte Schreibweise zur Verfügung. Man verstehe nämlich unter x die Matrix, welche aus der einen von den Vektorkomponenten $x_1, x_2, \dots, x_m$ gebildeten Spalte besteht; desgleichen ordne man die Zahlen $y_k$ in eine Spalte an; $A = \Vert a_{ki} \Vert$. Dann lassen sich nach der Komositionsregel [von Matrizen] die Gleichungen [...] zusammenfassen zu $y = Ax$``''``. - \cite[p. 10]{Weyl1928}

``The matrix calculus allows us to express the formul\ae for
a linear correspondence [...] in an abbreviated form.
We do this by denoting by $x$ that matrix whose only column consists of the
vector components $x_1, x_2, \cdots, x_m$; similar
for $y$. In accordance with the rule [...] for the composition of
matrices, equations [...] can be written $y = Ax$.'' - \cite[p. 8]{Weyl1931}
\end{quote}

Weyl writes $y = A x$ for the matrix-vector product, producing another vector.
Notably, German Gothic letters are not used. [\S 2, pp. 9-10]

\S p. 13 introduces the concept of ,,duale Vektorraum'' (``dual vector space'').
Weyl introduces ,,die \textit{duale} oder \textit{transponierte} $A^*$
(``the \textit{dual} or \textit{transposed} matrix $A^*$'')
as synonymous.
p. 14: Weyl writes for $x^\prime = A x$, the corresponding transformation $x^* = x^{\prime *} A^*$
in the dual spaces.
Thus Weyl introduces the concept of duality without dual vectors, but rather noting
that the entire matrix equation $x^\prime = A x$ can be transposed to yield the
dual mapping. Weyl does not mention explicitly that $x^*$ is now a row matrix, but it is
implicit in his description.~\footnote{There is also the possible confounding
factor of German--English translation, as ,,Reihe`` means both ``set'' and ``row''
in vernacular usage. The modern mathematical usage of ,,Reihe`` means ``series''.}

On p. 14 he writes
\begin{quote}
,,Wir haben ein für allemal verabredet, die Reihe der Komponenten $x_1, x_2, \cdots, x_n$
eines Vektors als eine Spalte aufzufasseb. Dann läßt sich das Produkt der beiden
Vektoren $\mathfrak x$ in $\mathfrak R$ und $\mathfrak x$ in $\mathsf P$
im Matrizenkalkül als $\xi^* x$ oder $x^* \xi$ schreiben.`` - \cite[p. 14]{Weyl1928}

``We have agreed once and for all to consider the set
$x_1, x_2, \cdots, x_n$ of components of a vector $\mathfrak x$ as a column; the
inner product of the vector $\mathfrak x$ in $\mathfrak R$ with the vector $\mathfrak x$
in $\mathsf P$ can therefore be written in matrix notation as $\xi^* x$ or $x^* \xi$.''
- \cite[p. 14]{Weyl1931}
\end{quote}

\S 4 introduces the ,,skalares Produkt`` (``scalar product''):
\begin{quote}
,,[Die] \textit{skalares Produkt} $(\mathfrak x \mathfrak y)$ zweir Vektoren
$\mathfrak x$ und $\mathfrak y$ [...] die zugehörige bilineare Bildung
$(\mathfrak x \mathfrak y) = \overline x_1 y_1 + \overline x_2 y_2 + \cdots + \overline x_n y_n $.``
- \cite[p. 15]{Weyl1928}

``[T]he corresponding bilinear form
$(\mathfrak x \mathfrak y) = \overline x_1 y_1 + \overline x_2 y_2 + \cdots + \overline x_n y_n $
[will be taken] as the \textit{scalar product} $(\mathfrak x \mathfrak y)$ of the two vectors
$\mathfrak x$ und $\mathfrak y$'' - cite[p. 16]{Weyl1932}
\end{quote}

\S 5, p. 20 - he writes $\alpha \cdot \mathfrak x$ for the scalar--vector product
that is the right hand side of the eigenvalue problem. The English edition omits the dot.

A very curious thing: ``outer or $\times$ m[ultiplication] of spaces, vectors,
operators'' shows up in the index of the English translation~\cite{Weyl1931},
but the word ``outer'' is absent on the cross-referenced pages both in the
English and German editions. The original German editions don't have an entry
with ``outer'' at all. So we must conclude that this was an invention of the
English translator, who is himself a notable American physicist.



\paragraph{Bocher (1931)}

Bocher 1931 appears to have the first inkling that matrices are themselves
quantities in their own right rather than simply a collection of elementary
quantities - section 21 on \textit{Matrices as complex quantities}



\paragraph{Turnbull and Aitken (1932)}

Emphasis on systematic notation for quadratic and bilinear forms as $x'Ay$.
To be distinguished from their earlier 1926 book, which does not adopt this notational convention.
They credit Cullis for his emphasis on the algebra of rectangular matrices. Although they do not credit him also for the notation, it is clear that his typographic conventions were adopted in their exposition.



\paragraph{MacDuffee (1933)~\cite{MacDuffee1933}}

preface mentions bilinear and quadratic forms (not sure if book does)
probably quoting eisenstein

``Denote by (x) the vector or one-column array'' - p10

``A matrix is often considered as a linear vector function.'' - p16

``A matrix may be interpreted as a linear homogeneous transformation in
vector space. From this point of view similar matrices represent the
same transformation referred to different bases. All the theorems of
this chapter may be interpreted from this standpoint.'' - p68

Starts with the abstract axioms of linear algebra and shows that the algebra
may be represented by naturally by matrices and matrix multiplication. Also
defines array as an ordered set with a known number of elements.

\paragraph{Wedderburn (1934)~\cite{Wedderburn1934}}

Starts with vectors as ordered sets of numbers as matrices as collections of
coefficients in systems of linear equations.

Bilinear form $A(x, y) = \sum^n_{i,j=1} a_{ij} \xi_i \eta_j $ (\S 1.07, p. 9).

The scalar product is a particular bilinear form, $S x y$ (\S 1.07, p. 9),
``where $A = \Vert \delta_{ij} \Vert = 1$''. He also extends the scalar product
to matrices (\S 5.02, p. 62), i.e.\ what we call today the Hadamard product, and
also to tensors (\S 5.16, p. 81).

Using the $S$-prefix notation for scalar product he also writes the bilinear
form as $A(x, y) = SxAy$.

He calls the outer product a ``tensor product'' (\S 5.09, pp. 72-73).

Alternates between ``vectors'' and ``hypernumbers''. On p72, \S 5.09, he
defined ``vector products'' as ``tensor product''. He attributed in Footnote 3
that Grassmann invented this product as the ``general product'' or
``indeterminate product''.

Follows \cite{Scheffers1889}.



\paragraph{King (1934)}

A chemistry paper from 1934 using the term ``outer product'' in
our modern usage, which notes that it is closely related to both Kronecker
products on matrices and the indeterminate product (dyad) of vectors. All the
citations in [1] refer to Kronecker products, with the original citation being
Zehfuss, 1858. [3] refers to Gibbs's terminology of dyad, also indeterminate
product.



\paragraph{Bartlett (1934)}

For the
inner or scalar product $\sum x_1 x_2 $
 one common notation is $(S_1 S_2)$; but
since we may regard $S_1$ as a matrix with one row, and $S_2$ the
transposed matrix of $S_2$ with one column, it will be convenient here
to write this product $S_1 S_2^\prime$, the usual matrix multiplication being
understood.

\url{http://dx.doi.org/10.1017/S0305004100012512}



\paragraph{Turnbull (1934)}

Turnbull 1934 appears to have popularized the notion of row/column vectors/matrices - he references Turnbull and Aitken (requested)

\begin{quote}
p14: \url{https://archive.org/stream/vectorsofmindmul010122mbp#page/n35/mode/2up}
A matrix that consists of a single column will be called a \textit{column vector}. A
matrix that consists of a single row will be called a \textit{row vector}. The
vectorial terminology is probably due to the fact that the elements in any
array may be regarded as the Cartesian coordinates of a point in a space of as
many dimensions as there are elements in the array. This point, together with
the original, determines a direction in space. In this manner any array of a
matrix can be given a vectorial interpretation.
\end{quote}

Turnbull also uses capitals for matrices and lowers for column vectors.

The idea of row*column = scalar product goes all the way back to Cayley! although he did not use these names and he spelled out the elements.
doi: 10.1098/rstl.1858.0002

\paragraph{Pipes (1937)~\cite{Pipes1937}}

A short review of matrix algebra. Matrix only; no vectors.


\paragraph{Bartlett (1938)}

Notation to simplify writing col vector, write them as row vector transposed

\begin{quote}
In conformity with orthodox notation, vectors may sometimes for clarity be
denoted by small letters, $x$, being a column vector, $x'$ a row vector
\end{quote}

\url{https://1988c0fbd715ddc21e4eab393a153f2278df184c.googledrive.com/host/0B8_joYJa2eNzfkNIQ2ZiY1hxS0FlMTI4ZC1IYnRUZ0ttd0NrQlYxcEdMZnFSMnpYZ3ltbUE/v34/1/S0305004100019897.pdf}

\url{http://dx.doi.org/10.1017/S0305004100019897}

Bartlett 1934 (referenced by 1938) did not use this notation, but did use the
notation $x^\prime y$ for inner product and also was among the first to use the
notion of vector transpose.

Other people who had the idea of a vector transpose:
Thomson, 1936
\url{http://psycnet.apa.org/journals/edu/27/1/37/}


\paragraph{Murnaghan (1938)~\cite{Murnaghan1938}}

A vectors first book. While matrices first appear on p2 as examples of a group,
their algebraic properties are not expounded upon. Instead, linear vector spaces
come first, then vectors as $n$-tuples.

He does something odd on p. 13:

\begin{quote}
    It is convenient to indicate the $n$ vectors $(u_1, \cdots, u_n)$ of a basis
    by the single symbol $\bf u$ so that $\bf u$ is a $1\times n$ matrix
    whose elements are \textit{vectors}. THen the equations $v_j = t_j^{\;a} u_a$
    which furnish the new basis $\bf v$ appear in the convenient form (note
    carefully the order of the factor matrices $\bf u$ and $T$)
    \[
    \bf v = \bf u T.
    \]
\end{quote}

Note that $\bf u$ is not considered a $m\times n$ matrix...

p. 18 - Structure of metric space is introduced with scalar product $(s|t)$.

p. 19 - Hermitian form $y^* H x$.

p. 24 - Only in \S7 on canonical forms does he then write $X$ for ``the $n\times n$
matrix whose columns are furnished by the components of the various vectors
$(x_1, \cdots, x_n)$''.

\todo{Go through bibliography}


\paragraph{Schwerdtfeger (1938)~\cite{Schwerdtfeger1938}}

A book about matrix functions.
Vectors come first as «$n$-uple»s (``$n$-tuples'') that represent points. \S 2, pp. 1-2, which discusses change of coordinates, he writes
\begin{quote}
«$y_1, \dots, y_n$ serait un autre $n$-uple par lequel on pourrait représenter le point $x$. Dans cette conception, le point x aurait le caractère d'un vecteur.»

``$y_1, \dots, y_n$ would be another $n$-tuple by which one could represent the point $x$. In this design, the point $x$ would have the character of a vector.''
\end{quote}

In \S 3, p. 2, he introduces the representations of points «par des symboles de colonnes» (``by columns of symbols'')
\[
x =\begin{pmatrix}x_1\\x_2\\:\\x_n\end{pmatrix}
\]
and a $n \times p$ matrix is introduces as «un système de $p$ colonnes» (``a system of $p$ columns'')

Here we also see the transposition of a vector, although it is not described as such.
\begin{quote}
«La matrice à une ligne, ayant les mêmes coordonnées que la matrice à une colonne $x$, sera désignée par

\[
x^\prime = \begin{pmatrix}x_1 & x_2 & \cdots & x_n\end{pmatrix}
\]

En enployant encore une fois cette opération de ``\textit{transposition}'' on sera ramenée au point $x$ ($= x^{\prime\prime}$).»

``
The matrix with one line, having the same coordinates at the matrix with one column $x$, will be designated by[...]

Employing once again this operation of ``transposition'' one will reduced to the point $x$ ($= x^{\prime\prime}$).''

\end{quote}
Here we see the notion that the transpose is idempotent, i.e. $x = x^{\prime\prime}$.
In \S 8 we see mention of Le «\textit{produit intérieur} (ou \textit{scalaire})» as defined as the number $x^\prime y$ defined in \S 3. Le «\textit{produit extérieur}» refers to the cross product and there is a discussion of the representation of the exterior product in the algebra of skew-symmetric matrices.

Citations:

von Neumann, Allgemeine Eigenwerttheorie Hermitesche Funktionaloperatoren
Math Ann 201 1929 49-131 (A = A' as projection operators)

cites the use of fundamental notions of modern algebra and algebraic numbers

Hasse, Hohere Algebra I, Samml  Goeschen 931, 1926

O Ore, Les corps algébriques et al théorie des idéaux, Mémorial des Sc. Math 64 1934


\paragraph{Aitken (1939)~\cite{Aitken1939}}

Matrices first. Column and row vectors defined synonymously with their matrix equivalents.
of orders $n\times1$ and $1\times p$.
column vectors are $x = \{x_1\;x_2\;\dots\;x_n\}$,
row vectors are $u = [u_1\;u_2\;\dots\;u_p]$.

\S 4, p. 5

\S 10 - bilinear, qudratic and Hermitian forms $x^\prime A y$.

No mention of inner or outer products.


\paragraph{Margenau \& Murphy~\cite{Margenau1943}}

An influential textbook ``Mathematics of Physics and Chemistry'' defines arrays
effectively as matrices without linear algebra~\cite[\S 10.1, p. 288]{Margenau1943}:

\begin{quote}
A collection of real or complex quantities is called an
\textit{array} if it can be displayed in an orderly table of \textit{rows} and \textit{columns}.
\end{quote}

They consider row and column vectors as matrices as determinants as a particular kind of array.

A matrix is typeset in bold italic capital Roman, $\boldsymbol A$.
A vector is typeset in bold small Roman, $\textbf u$ or enclosed in braces, $\{ \textbf u \}$.
Row are $[\textbf u ]$.
Kronecker product of matrices is the direct product.
Transpose is $\tilde{\boldsymbol A}$.
Scalar product (10-11), $\tilde{\mathbf x} \mathbf y$ or $[\mathbf x] \{\mathbf y\}$ (10-7).
Hermitian scalar product is $\mathbf x^\dagger \mathbf y$



(Notes above are for second edition)



\paragraph{Ingram (1944)}

Outer products

TODO \url{http://www.jstor.org/stable/3029991}

TODO \url{http://www.jstor.org/stable/2307548}

In
\url{http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1078730/}

they explain their notation is a variant of Birkhoff and Langer, 1923

Row vector $\cdot v$, column vector $u:$

Inner product is $\cdot v u:$

Outer product is $u:\cdot v$



\paragraph{Schwerdtfeger (1945)~\cite{Schwerdtfeger1945}}

A vector in the three-dimensional space will
be represented by a column matrix...

where $xy'$ is the column-row product (representing
the often so-called ``dyadic product'' of the
two vectors $x$, $y$),

references \cite{Schwerdtfeger1938,Murnaghan1938,Pipes1937}

TODO \url{http://www.jstor.org/stable/2303007} (1944)

TODO Householder 1950 \url{http://www.jstor.org/stable/2308297}

\paragraph{Duncan and Collar (1934)\cite{Duncan1934}}

Describe the solution of a differential equation in terms of the eigenvalues of
a matrix representation. The eigenvalues are found by repeated power iterations.

This paper is ``matrices only''. Columns are described as matrices.
The significance of the resulting column matrix from power iteration as
eigenvectors is mentioned as ``"``amplitude ratios''.
Only the implications for finding ``latent roots'' (eigenvalues) are discussed.
The treatment here references earlier work on principal axes, but differs from
the purely geometrical approach by making use of matrix algebra.

\paragraph{Frazer, Duncan and Collar (1938)~\cite{Frazer1938}}

The book cited by Householder as containing details of numerical computations.

Matrices came first, then row and column matrices.

A curious footnote on p. 2 says that
\begin{quote}
    A row matrix is often called a \textit{line matrix}, \textit{a vector of the first kind},
    or a \textit{prime}; while a column matrix is referred to as \textit{a vector of the second kind}, or a \textit{point}.
\end{quote}

While the terms ``vector of the first/second kind'' appear in the early descriptions
of general relativity from the 1910s, these refers to polar and axial vectors
(vectors and pseudovectors) respectively. Since pseudovectors are formed by
exterior products of vectors, the resulting dimensionality of the vector space
is in general different from that of the original vector space.
Thus, it is unclear what the origins of these names are.

Actually it's probably from Aitken and Turnbull 1932.


\paragraph{MacDuffee (1943)~\cite{MacDuffee1943}}

Cites \cite{Wedderburn1934}.

\todo{Also cites Schreier and Sperner 1932. Requested from library}

``Vectors in ordinary space'' come first, presenting 3D vectors. Then ``vectors
in general'', in the $n$-dimensional setting. \S 7, p. 17 says

\begin{quote}
    We define an ordered set
    \[
        \phi = (a_1, a_2, \cdots, a_n)
    \]
    of $n$ numbers of a field $F$ to be a \textit{vector} of \textit{order} or
    \textit{dimension} $n$, and call $a_1, a_2, \cdots, a_n$ its components.
\end{quote}

The inner product is $\phi\cdot\phi_1$.

\begin{quote}
\S p. 23. - A \textit{matrix} may be thought of as an ordered set of vectors. [...]
If $A$ is an $m\times n$ array, it consists of $m$ row vectors, each of order $n$.
If $m=1$, $A$ is an ordinary vector of order $n$, so that the concept of matrix
generalizes the concept of vector.

p. 25 - A matrix may alternatively be thought of as conssiting of a row of $l$
columns, each column being a vector...
\end{quote}

Even here we see that the distinction between vector and (row) matrix is not
clearly made.

Description of matrix multiplication:

\S pp. 26-7.
\begin{quote}
    We define, then, the product of two matrices $A$ and $B$, the first composed of
    row vectors of order $m$, the second of column vectors of order $m$, to be the
    matrix whose element in row $r$ and column $s$ is the inner product of the $r$th
    row vector of $A$ by the $s$th column vector of $B$.
\end{quote}

At the end of the book the notion of vector space is introduced and Theorem 88
(p. 182) proves that ``every finite vector space of dimension $n$ is isomorphic with a
vector space of $n$-tuples of numbers of $F$ as defined in \S7.''
(Square) matrices are introduced similarly in Theorem 89 (p. 185) as being
isomorphic to the endomorphism ring of a vector space of dimension $n$.

\paragraph{Von Neumann and Goldstine~\cite{VonNeumann1947} (1947)}

The famous paper about numerically inverting a matrix.

p. 1041: Matrices $A = (a_{ij})$, vectors $\xi = (x_i)$

Matrix products and inner products are written out explicitly in component notation
...

p. 1042: Mixed associative law, $A(B\xi) = (AB)\xi$.

p. 1044:

For $A=(a_{ij})$ fix $j=1,\dots,n$ and view $i=1,\dots,n$ as a vector index, then $A^{\{j\}} = (a_{ij})$ ($i=1,\dots,n$) defines a vector $A^{\{j\}}$.

p. 1045:

They introduce a zero-left-padded notion of a matrix associated with the vector

Given a vector $\xi = (x_i)$ $(i=1,\dots,n)$ define a matrix

\[
\xi^\sim = (x_{ij}) \qquad x_{ij} = \begin{cases}
x_i\textrm{ for }j=1,\\
0\textrm{ for }j\ne1.
\end{cases}
\]

p. 1044




\paragraph{Bourbaki (1947)}
\url{https://archive.org/stream/ElementsOfMathematics-AlgebraPart1/Bourbaki-ElementsOfMathematicsAlgebraPart1#page/n237/mode/2up}

\paragraph{Wade (1948)~\cite{Wade1948}}

Cited in second edition of \cite{Margenau1943}.

Uses Cullis's notation $A = {[a]}^m_n$ for matrices.
Vectors come first as $n$-tuples with vector space structure.
Vectors are $\alpha$, matrices are $A = {[a]}^m_n$
inner product is $\alpha \cdot \beta$, synonymous with dot product, scalar product, direct product.
outer product and forms absent.
in Ch. 5, matrices, vectors are introduces as constituents of matrices
\S 5.4 p 36 - Matrices as ordered sets of row vectors (primarily) or column vectors (secondarily).
\S 5.5 p 37 $A = {[a]}^1_n = (a_1, a_2, \dots, a_n)$ is a row matrix,
$A = {[a]}^m_1 = \{a_1, a_2, \dots, a_m \}$ is a column matrix. (Also appears vertically)
\S 5.5 p 38 - matrix transpose is $A^\prime$.
\S 5.5 p 39 - the transpose of a column vector is a row vector.
\S 5.5 p 39 - Column vectors are $\alpha$.



\paragraph{Manchester (1948)}

The Manchester Mk I had the concept of a B-line, a memory offset register, Ref 28?



\paragraph{Ritt (1950)~\cite{Ritt1950}}

First use of the term ``structure constant''?

\url{http://www.jstor.org/stable/1969444?seq=1#page_scan_tab_contents}


\paragraph{Perlis (1952)~\cite{Perlis1952}}

Matrices first. Column and row vectors defined synonymously with their matrix equivalents.

Column vectors are $X = \textrm{col }(x_1, \dots, x_s)$ - p. 4, \S 1.2.
\S 1-4, p. 9 - scalar product is scalar-matrix product, $c A$.

Vector spaces come in Ch. 2. Vectors as $n$-tuples. are small Greek letters.

\begin{quote}
Thus ordered $n$-tuples will be regarded here as objects which may be
represented as rows, or columns, whichever may be convenient at the moment.

p. 24, \S2-2.
\end{quote}



\paragraph{Milner (1952)}

\url{http://rspa.royalsocietypublishing.org/content/214/1118/312}
is apparently the first to express the ``matrix product'' of two vectors,
expressed as $a \overline{a}$.  The terms scalar product $\overline{a} b$ and
matrix product are introduced.



\paragraph{Householder (1953)~\cite{Householder1953}}

The first book on linear algebra focusing on numerical algorithms.

Householder is famous for insisting on a systematic notation for various
quantities.  The convention for matrix variables written in capital Roman
letters, vectors in small Roman letters, and scalars in small Greek letters was
popularized by him. Note however that the works of Cullis already used such a
systematic convention.
There is also the superscript $T$ for transpose, with expressions like $x^T y$
and $1 - x y^T$.  Although Householder never uses the term ``outer product'',
we see expressions like $D - u v^T$ in the book, so we see diagonal plus rank
one terms everywhere. He does not present $u v^T$ as an expression deserving of
its own identity.

The book begins with vectors first. Even further, Householder starts with
vectors as abstract elements of a vector space, expanding them in a basis
$\mathbf e_k$ spanning the space (although he does not use these terms) as
$\mathbf x = \sum_k \xi_k \mathbf e_k$ and then calls the $n$-tuple of
coefficients $\xi_i$ a ` ``numerical vector'' $x$'. But then he proceeds to
arrange these coefficients in a column and also call that ``a (numerical)
vector'' (without quotation marks).
Householder's convention here is to distinguish  ``geometrical vectors'' like
$\mathbf x$ from ``numerical vectors'' like $x$. In modern terms, we might say
that the former are the abstract, coordinate-free representation, and the
latter are the concrete coordinates for a particular choice of basis.
Householder's thinking clearly reflects that he thinks of numerical vectors in
the ``matrix first'' school while geometrical vectors belong to the ``vectors
first'' school. So even here we see an incomplete fusion of the two schools of
thought.

On p. 20 Householder does something strange. He writes down a quantity (2.01.13), p. 20:
\[
\mathbf e = \begin{pmatrix}\mathbf e_1 & \mathbf e_2 & \cdots & \mathbf e_n\end{pmatrix}
\]
as describes it ``formally as though it were a (numerical) row vector''.
Householder does not say that $\mathbf e$ is a matrix.

p. 22 - Householder uses ``outer product'' in Grassmann's sense, written as
$[\mathbf a, \mathbf b]$.

On p. 26 Householder defines the matrix transpose $M^{\mathsf T}$, and immediately
after that writes ``In particular, if $x$ is the column vector of the $\xi_i$,
$x^{\mathsf T}$ is the row vector of the $\xi_i$.'' Clearly he means that the
transpose of a vector is a special case of the matrix transpose.

He also writes, curiously, a metric matrix $G = (\gamma_{ij}) = \mathbf e^{\mathsf T} \mathbf e$,
where $\gamma_{ij} = \mathbf e_i \mathbf e_j$ and $\mathbf e$ is the formal row
vector of column vectors.

p. 36 - in the discussion of the eigenvalue problem, Householder introduces a
transposed (numerical) vector for the first time, in the expression $g^{\mathsf
T}Tf$ in (2.06.23).



\paragraph{Bodewig (1956/9)~\cite{Bodewig1956}}

Vectors come first, but $n$-tuples don't actually play a part in the exposition.
Instead, they are glossed over immediately in favor of column vectors from the
``matrices first'' school:

\begin{quote}
A vector $\mathbf v$ of order $n$ is an ordered set of $n$ numbers (so-called
components) $v_1, v_2, \dots, v_n$, whose succession is significant. All $v_i$'s are here
assumed to be \textit{real}, in general.

The numbers may be arranged vertically, i.e.\ in the form of a so-called
column, in which case they are called a column vector or briefly a
column and are denoted by $\mathbf v$. If the numbers are arranged horizontally,
i.e.\ in the form of a row, they are then called a row vector or briefly a
row and are denoted by $\mathbf v^\prime$.

The product $\mathbf v^\prime \mathbf w$ of a row $\mathbf v^\prime$ and a column
$\mathbf w$ (of the same order $n$) is[...] called the scalar product of the two. It is a number.

- 2/e (1959), p. 1
\end{quote}

Matrices come in chapter 2, as does the outer product $\mathbf v \mathbf w^\prime$
on p. 10 (2/e), called a ``simple product''.

This book is also notable for its preface, explaining the author's preference a
systematic calculus of multiplication using basis vectors $\mathbf e_k$ to denote
operations like extracting the $k$th row of $\mathbf A$ as the product $\mathbf e_k A$.
p. 5 introduces basis vectors, $\mathbf e_k$, which he calls ``unity vectors',
and on p. 10 the vector of all ones, $\mathbf e$. With these vectors he points
out that you can write ``sum column'' and ``sum row'' vectors of $\mathbf M$. The
former he writes as:
\[
\mathbf{Me} = \mathbf M_{.1} + \mathbf M_{.2} + \dots + \mathbf M_{.n},
\]
being the column vector containg the sum over all the columns, and similarly for
the ``sum row'' vector $\mathbf{e^\prime M}$. In modern slicing notation we might
write \verb|M[:,1]| or even $M_{:,1}$ for $\mathbf M_{.1}$.

On p. 13 Bodewig states that $\mathbf e^\prime_i \mathbf A = \mathbf A_{i.}$
extracts the $i$th row of $\mathbf A$, and $\mathbf A \mathbf e_k = \mathbf A_{.k}$
extracts the $k$th column of $\mathbf A$, and furthermore that
$\mathbf e^\prime_i \mathbf A \mathbf e_k = a_{ij}$.

On p. 14 Bodewig points out that unit matrices $\mathbf E_{ik}$, which previously
appeared in abstract algebraic treatments, can be expressed ``as simple products
of two unit vectors'', i.e.\ $\mathbf E_{ik} = e^\prime_i e_k$.

Bodewig is very proud of his notation, saying on p. 13 that
``[o]nly by using the formulae [above] will the calculation with matrices become a calculus.''

Bodewig presents bilinear and quadratic forms as $\mathbf{x^\prime A y}$ and
$\mathbf{x^\prime A x}$ respectively in chapter 5, p. 47.



\paragraph{Faddeev and Faddeeva - \cite{Faddeev1959}}

Matrices first.



\paragraph{Schreier and Sperner (1931)~\cite{Schreier1931}}

Vectors first.
Vectors are fraktur, $\mathfrak a = \{a_1, a_2, \cdots, a_n\}$
Scalar product $\mathfrak a \cdot \mathfrak b$. - end of \S 2

\S6 Associate a vector with each column of [a] matrix - \S 6

No mention of forms, inner product or outer product.


\paragraph{SODA (1959)}

I also found a 1959 paper about the English Electric DEUCE which
was first produced in 1955. This paper describes a programming
language called SODA, which has the concept of an array (see Figure
1). Notably, the paper devotes a paragraph to explaining what an array
is, which in modern language is a collection of data stored
contiguously on a memory drum. This does not predate the 1954 and 1958
dates for FORTRAN and ALGOL, but so far it is the earliest date for a
paper that explains what an array is that I can find:

\url{http://research.microsoft.com/en-us/um/people/gbell/CGB%20Files/Translation%20Routine%20for%20the%20Deuce%20Computer%20July%201959%20c.pdf}


\paragraph{Finkbeiner (1960)~\cite{Finkbeiner1960}}

Starts with sets and abstract algebra (and even a little bit of category theory).

Vector spaces come first, then vectors over $R^n$ as $n$-tuples. (p. 27)

Real numbers also trivially form a vector space (p. 27)

A linear algebra is then defined (p. 49) the combination of a vector space and a composition operator for linear maps over the vector space.

p 26. [A] vector space is somewhat more complicated that the abstrat systems in the first chapter, principally because there are two sets of elements involved. One set $V$ consists of abstract elements called \textit{vectors} and are denoted by small Greek letters; the other set $F$ is a field whose elements are called \textit{scalars} and are denoted by small Latin latters. In addition to the two field opeations, $+$ and $\cdot$, two operations in volving vectors are postulated: the ``addition'' of vectors, denoted $\alpha \oplus \beta$, and the ``multiplication'' of a scalar and a vector, denoted $a \oplus \alpha$.


Definition 2.1. A system $\mathcal V = \{V, F; +, \cdot, \oplus, \odot\}$ is called a \textit{vector space over the field} $\mathcal F$ if and only if
(a) $\{F, +, \cdot\}$ is a field $\mathcal F$ whose identity elements are denoted by 0 and 1,
(b) $\{V; \oplus\}$ is a commutative group whose identity element is denoted $\theta$,
(c) for all $a,v\in\mathcal F$ and all $\alpha, \beta \in V$, $a \odot  \alpha \in \mathcal V$ and
(i) $(a + b) \odot \alpha = (a \odot \alpha) \oplus (b \odot \alpha)$,
(ii) $a \odot (\alpha \oplus \beta) = (a \odot \alpha) \oplus (b \odot \beta)$,
(iii) $(ab) \odot \alpha = a \odot (b \odot \alpha)$,
(iv) $1 \odot \alpha = \alpha$.

``Fortunately, the notation used above can be simplified. From the type of letters involved it is clways clear whether we are adding two scalars or two vectors, or multiplying two scalars or a scalar and a vector; therefore, we can use $+$ to indicate both types of addition and juxtaposition to indiate both types of multiplication.''

This description of a vector space is interesting because it tries to disentangle the different defintions of addition and multiplication when discussing vectors, and then conflates the notation again, noting that the types of the operands determine which definition is to be used.

Definition 3.4. A \textit{linear algebra} $\mathcal L$ over a field $\mathcal F$ is a system
\[
\mathcal L = \{L, F; +, \cdot, \oplus, \odot, \boxdot\}
\]
which satisfies the postulates:
(a) the system $\{L, F; +, \cdot, \oplus, \odot\}$ is a vector space over $\mathcal F$,
(b) $\boxdot$ is a binary operation on $\mathcal L$ which is closed, associative, and bilinear.

$\mathcal L$ is also a vector space over $\mathcal F$.


p. 71: Row and column vectors are introduced under \S 4.2, Special Types of Matrices.

Relative to a fixed basis, every vector of $\mathcal V_n$ has a unique representation as an $n$-tiple of scalars, $(a_1, \dots, a_n)$. Except for the presence of commans, this is formally the same as a matrix of one row and $n$ columns. Accordingly, a $1\times n$ matrix is called a \textit{row vector}. The transpose of a row vector is an $n \times1$ matrix, $n$ rows and one column, and is called a \textit{column vector}. If $A$ is a reow vector, where $A = \begin{pmatrix} a_1 & a_2 & \ldots & a_n\end{pmatrix}$, and if $B^\prime$ is a column vector, where $A = \begin{pmatrix} b_1 & b_2 & \ldots & b_n\end{pmatrix}$, then both $AB^\prime$ and $B^\prime A$ are defined:

\[
AB^\prime = \begin{pmatrix} a_1 & a_2 & \ldots & a_n\end{pmatrix}
            \begin{pmatrix} b_1\\ b_2\\ \vdots\\ b_n\end{pmatrix}
          = \begin{pmatrix} a_1b_1 + a_2b_2 + \ldots a_nb_n\end{pmatrix},
\]

which is a $1\times1$ matrix, or by Exercise 5, \S 4.1, a scalar[.]

Exercise 5 in question (p. 67) is:

Show that the system of $1\times1$ matrices over a field $\mathcal F$, together with matrix addition and multiplication, is a field which is isomorphic to $\mathcal F$.

This again speaks to the importance of recognizing isomorphisms...

\paragraph{Gantmacher (1960)~\cite{Gantmacher1960}}

\cite[p. 280, Definition 10]{Gantmacher1960} defines the transpose to have the following properties:

For a linear operator $A$ over $\mathbb R^n$,

the transpose $A^T$ is the operator such that for any two vectors $x, y \in \mathbb R^n$,

\[
\left\langle Ax, y \right\rangle = \left\langle x, A^T y \right\rangle
\]

and has the properties

\begin{align}
{(A^T)}^T & = A \\
{(A + B)}^T & = A^T + B^T \\
{(\alpha A)}^T & = \alpha A^T \textrm{ for all real numbers } \alpha \\
{(A B)}^T & = B^T A^T
\end{align}

Analogous definitions and properties for the $A^*$, the adjoint of $A$ (for operators over unitary space $\mathbb C^n$), appear on p. 266.

Gantmacher never uses the terms ``row vector'' or ``column vector'', but defines row and column matrices as matrices having only one row or column respectively. Vectors are defined only as elements of a vector space \cite[p. 51]{Gantmacher1960}, having the usual axiomatic properties of vector spaces over a field $\mathbb F$ and addition and multiplication. He then defines a column as a $n$-tuple of numbers (p. 51, Example 2), $\mathbb F^n$, and shows that operations defined as ``operations on column matrices'' fulfill the axioms of a vector space.

Gantmacher also distinguishes \cite[pp. 54-56]{Gantmacher1960} between the linear operator $A$ and its matrix representation $A$, saying that the $k$th column of the matrix ``consists of the coordinates of the vector $Ae_k$''.

``If in an $n$-dimensional space a basis $\mathbf e_1, \mathbf e_2, \dots, \mathbf e_n$ has been chosen, then to every vector $\mathbf x$ there corresponds uniquely the column $x = (x_1, x_2, \dots, \mathbf x_n)$ where $x_1, x_2, \dots, \mathbf x_n$ are the coordinates of $\mathbf x$ in the given basis. Thus, the choosing of a basis establishes a one-to-one correspondence between the vectors of an arbitrary $n$-dimensional vector space $\mathbf R$ and the vectors of the $n$-dimensional number space $\mathbb F^n$... This means that to within isomorphism there exists only one $n$-dimensional vector space of a given number field.''

The fuss over the abstractions of vector space may seem fussy, but in fact it the definition of matrix multiplication arises naturally from the composition of linear operators, and is correct independent of basis. The notion of basis-independent properties is what is often neglected in computer representations, as is the notion of ``up to isomorphism'' as described by Gantmacher.



\paragraph{Hockney (1961)}

A 1961 paper by Hockney in the ALGOL Bulletin

\url{http://archive.computerhistory.org/resources/text/algol/algol_bulletin/AS12/AS12.HTM}

describes the modern concept of the array data structure as describing
both elements and shape. Para. 3.7:

``This concept of <array type> requires that an array carries with it a
statement of its dimensions and the dimensions of any of its elements
that may themselves be arrays... One should think of an array as the
totality of the elements of the array together with all the bound pair
lists describing the dimensions of the array.''

The term ``array'' appears to have evolved from its vernacular usage in ``array of cobscripted variables''

Has the first description of a multidimensional array as parametric on indexing quantities and the element type.




\paragraph{Iverson (1962)~\cite{Iverson1962}}

Possibly noteworthy as the first publication of a computer program for numerical linear algebra using vectorized operations, as implemented in APL.
Resurrects (?) the outer product in a paper describing the implementation of Gauss-Jordan elimination using vectorized APL operations. The description of Program 2 (and its accompanying footnote) states that

\begin{quote}
`[S]tep 8 subtracts from $\underline{\textit{M}}$ the outer product of the first row (except that the first element is replaced by zero) with its first column. The outer product $\underline{\textit{Z}}$ of a vector $\underline{\textit{x}}$ by vector $\underline{\textit{y}}$ id the ``column by row product'' denoted by $\underline{\textit{Z}} \leftarrow \underline{\textit{x}} \times \underline{\textit{y}}$ and defined by $\underline{\textit{Z}}^{\; i}_j = \underline{\textit{x}}_i \times \underline{\textit{y}}_j$.'
\end{quote}

This paper cites an ALGOL implementation of Gauss-Jordan elimination, but this algorithm never computes the outer product explicitly, instead computing the outer product on the fly as part of the update of the matrix:~\cite{Cohen1961}

\begin{quote}
\begin{verbatim}
a[k,j] := a[k,i] - b[j] x c[k]
\end{verbatim}
\end{quote}

Iverson does not claim that his APL version is equivalent to the ALGOL one, but none of his references ever refer to the $b c^\prime$ matrix as the outer product of vectors.

It is fair to say that by 1962, Iverson recognized the quantity $b c^\prime$ as having its own existence outside of the diagonal+rank1 expression $I - b c^\prime$, and that he did so independently of anyone else. In this paper and the APL book Iverson meant the notation to be useful for both machine computation and analysis by hand, so it is unclear if Iverson ever intended $b c^\prime$ to be constructed explicitly on a machine. (It seems unlikely given the context of matrix inversion.) Whether or not anyone else noticed before that is debatable - Rutishauser didn't in 1959.

(Curiously, the APL book does not mention the phrase ``outer product'', despite having it in the index!~\cite{Iverson1962book})


\paragraph{Wilkinson's ``The algebraic eigenvalue problem'', 1965}

Did he have anything notationally significant?



\paragraph{Forsythe and Moler (1967)~\cite{Forsythe1967}}

They present the vector transpose on p. 2.
\begin{quote}
``Let $x = {(x_1, x_2, \dots, x_n)}^T$ denote a column vector in real $n$-dimensional space $R^n$. Let $x^T$ denote the row vector which is the transpose of $x$.''~\cite[p. 2]{Forsythe1967}
\end{quote}

Although they cite \cite{Faddeev1959} for the introductory material, \cite{Faddeev1959} itself is firmly rooted in the ``matrices only'' tradition, with column vectors treated as synonymous with column matrices.



\paragraph{Strachey (1967)~\cite{Strachey1967}}

By the time computer scientists thought of formalizing the grammar and semantics of programming languages,
vectors and arrays are considered well-understood data structures. \cite[\S 3.7.7, pp. 43--44]{Strachey1967} writes:
\begin{quote}
Vectors and arrays are reasonably well understood. They are parametric types so that the type of an array includes its dimensionality (the number of
its dimensions but not their size) and also the type of its elements[...]
all the elements of an array have to be of the same type, though their number may vary dynamically.
It is convenient, though perhaps not really necessary, to regard an $n$-array
(i.e., one with $n$ dimensions) as a vector whose elements are ($n-1$)-arrays.
We can then regard the R-value of a vector as something [that] gives access (or points to) the elements rather than containing them.
\end{quote}
We recognize herein the modern description of the array data type being defined by
two parameters: the number of dimensions and the type of elements.

Does Stachey assume integer indexes?



\paragraph{Fischer (1973)~\cite{Fischer1973}}

Presents the abstract algebra version with rings first, then vector spaces of

p 12-3

0.3 Die Vektoren des n-dimensionalen Raumes.

In der ``Ebene'' $E_2$ wähle man ein (rchtwinkligen) ``Koordinaten-system''. Sei $P$ ein beliebiger ``Punkt'' der Ebene $E_2$.

Ein Element von $R^n$ bezeichnet man mit $x = (x_1, ..., x_n)$, wobei $x_i \in R$ für $i = 1,..., n$.

Für die Elemente von $R^1$, $R^2$ bzw.\ $R^3$ schreibt man stattdessen auch $x$, $(x,y)$ bzw.\ $(x,y.z)$.
Die Elemente von $R^n$ nennt man ``Punkte'' oder ``Vektoren''

- pp 12-13

Matrices appear on p. 54 as representations of bases of vector spaces

- 1.3 Praktische Bestimmung von Basen

Untr einer \underline{Matrix} (aus Elementen eines Körpers $K$)  versheht man ein rechteckiges Zahleschema...

Matrices The interoduce row and column vectors

Unter dem $i$-ten \underline{Zeilenvektor} von $A (i=1, ..., m)$ versteht man

\[
a_i := (a_{i1}, ..., a_{in}) \in K^n
\]

unter dem $j$-ten Spaltenvektor von $A (j=1, ..., n)$

\[
a^j := (a_{1j}, \vdots, a_{mj}) \in K^m.
\]

($K$ being the ring over which the vector spaces is being defined.)

On p. 60 the notion of a matrix being a column vector of row vectors is written out explicitly when introducing the notion of row echelon form:

1.3.9. Korollar:

Seien $v_1, ..., v_n \in K^n$ gegeben. Dann gilt:
$(v_1, d..., v_n)$ ist genau dann eine Basis von $K^n$, wenn sich die $n\times n$-Matrix $A = (v1, \vdots, v_n)$ (mit denn $v_i$ ($i=1,...,n$) als Zeilenvektoren)
durch endliche viele elementare Zeilenumformungen in eine Dreiecksmatrix mit lauter von 0 verschiedenen Diagonalelementen überführen läßt.

Later matrices are revisited as representations of lienar maps

The transpose is ${}^t A$ (p. 85). The prime notation $a'$ shows up in the ordinary sense of "a new quantity related to a".

Under der \underline{Transposition} der Matrizen aus $M(m\times n; K)$ versteht man die Abbildung

\[
t: M(m\times n; K) \rightarrow M(n \times m; K) \\
A = (a_{ij}) \mapsto {}^t A = (a^\prime_{ji})
\]
mit $a^\prime_{ji} := a_{ij}$ für $i=1,...,m, j=1,..., m$

---

Matrix multiplication is introduced on page 86.

On p 87 we see explicit conflation of scalars and $1\times1$ matrices

\[
(a_1...a_n) \cdot (b1 \vdots bn) = a_1 b_1 + \dots + a_n b_n \in K = M(1\times 1; K)\]

In diesem Fall kann man $A$ und $B$ als Vektoren aus $K^n$ auffassen. Für $K=R$ ist dann $A\cdot B$ nichts anderes als das in (0.4.1) definitere Skalarproduct von $A$ und $B$.

Primary influence

Serge Lang: Linear Algebra, 1969

Bibliography

Bourbaki: Algèbre linéaire (Éléments de Mathématique, Première Partie, Livre II, Chapitre II). 1962

Godement, R: Cours d'algèbra. 1966

Greub, Linear Algebra. Springer 1969
Koecher: Lineare Algebra. orlesungsausarbeitung München 1969

Kowalsky: Lineare Algebra 1969

Lingenberg,: Lineare Algebra. Mannheim 1969


\paragraph{Stewart (1973)~\cite{Stewart1973}}

Vectors come first, but they have distinct columnness. In particular, p. 2:

\begin{quote}
    \textbf{Definition 1.1.} A $n$-vector $x$ is a collection of $n$ real numbers
    $\xi_1, \xi_2, \dots, \xi_n$ arranged in order in a column:
    \[
        x = \begin{pmatrix}\xi_1\\\xi_2\\\vdots\\\xi_n\end{pmatrix}.
    \]
\end{quote}

p. 21 - Definiion 3.1 An $m\times n$ \textit{matrix} is a rectangualr array of numbers
having $m$ rows and $n$ columns.

p. 21 - Example 3.2. The $n\times1$ matrix $A$ [...] looks exactly like a member of $\mathbb R^n$. In this book
we shall not distinguish between $n\times1$ matrices and $n$-vectors; they will be denoted by
upper or lower case Latin letters as convenience dictates.

p. 21 - Example 3.3 The $1\times n$ matrix $R$ has the form
\[
    R = (\rho_{11},\rho_{12},\dots,\rho_{1n})
\]
Such a matrix will be called a \textit{row vector}

We see here an asymmetry in the indexing conventions between row vectors and
column vectors.

\paragraph{Wirth (1973)~\cite{Wirth1973}}

The book ``Structured Programming'' is possibly the first major textbook on theoretical computer science, as it introduces the study of data structures and algorithms as a discipline separate from mathematics.

Wirth uses a pseudocode notation inspired by ALGOL 60. He also credits Rutishauser
in the preface as ``the originator of the idea of programming languages''.

This book is notable for Ch. 11, about the array data structure, and Ch. 8, on data types and types.

Types: Wirth introduces p44 that ``The introduction of a new variable, in particular,
should be accompanied by the specification of
its range of possible values.'' Wirth states that doing so is necessary to understand
how an algorithm works and its correctness, and further allows reasoning about memory requirements
of efficient representations.
Wirth defines ``type'' informally as ``[t]he set of values that a variable may
assume''. p45 also introduces the concept of ``scalar value'' as a value
which is ``unstructured (i.e.,\ not decomposable into components.)''


Wirth defines the array data structure as `` a collection of \textit{component
variables of the same type}'' - Ch. 11, p. 80, being one of his primary
examples of a structured type. As distinct from a file, ``Each single component
of an array is explicitly denotable and directly accessible''.\footnote{Wirth
also says that the number of
components of an array is fixed once initialized, which is something we don't
consider to be an essential characteristic of arrays anymore in the context of
mutable lists.}

Wirth says that an array type must specift rhe componen type and the index type,
generalizing Strachey's definition to introduce possibly non-integer indexes.

p 83 - gives as an example a program to compute the scalar product.
Wirth does not refer to the arrays as vectors, but simply lists the elements of
both vectors, each with a single subscript.

p 87 - ``Components of arrays need not be scalars---they themselves may be structured.
If they are again arrays, then the orrignal array $A$ is called \textit{multi-dimensional}.
If the components of the componet arrays are scalars, then
$A$ is called a \textit{matrix}.'' Wirth clearly defines arrays recursively.



\paragraph{Golub and Van Loan (1983)~\cite{Golub1983}}

By the time the first edition of \textit{Matrix Computations} was published,
the notions and notations of numerical linear algebra had been firmed up in the
form familiar to all in the field. Householder notation (capital Roman letters
for matrices, small Roman letters for vectors and subscripted components of matrices and vectors,
and small Greek letters for scalars, superscripted $^T$ for transpose) was widely
if not exclusively, adhered to.

Notably, matrices are introduced first in the book.
Vectors are introduced on the second page as follows:
%
\begin{quote}
$\mathbb R^m$ denotes $\mathbb R^{m\times1}$, and for these column vectors we customarily use
lowercase letters and denote individual components with single subscripts.
- \cite[p. 2]{Golub1983}
\end{quote}
%
so clearly they still adhere to the ``matrices first'' school, thinking of both
inner and outer products as special cases of matrix--matrix products.

\todo{what do the second and third editions say?}

By the fourth edition of the book they still adhere to this school of though,
with the additional clairification
%
\begin{quote}
Notice that we are identifying $\mathbb R^n$ with $\mathbb R^{m\times1}$ and so the members of $\mathbb R^n$ are
\textit{column} vectors. On the other hand, the elements of $\mathbb R^{1\times n}$ are \textit{row} vectors[...]
- \cite[p. 3]{Golub2013}
\end{quote}
%

\paragraph{Becker and Chambers (1984)~\cite{Becker1984}}

p. 33

The fundamental data struture [of S] is a \textit{vector}; that is, a seuqnce of data values (numbers, logical values or character strings).

p. 124

Vector structure

Most numerical functions operating element-by-element (e.g., \textbf{abs}, \textbf{sin}) do the obvious: they produce a vector structure with the data values changed and the structure unchangeg. However, some functions produce a vector without structure, regardless of the structure associated with the argument. All the sorting functions have this property. For example, sorting the data values in a matrix should not (and does not) produce a matrix.

p. 444 - The result [of \textbf{sort(data)}] is a vector, even if \textbf{data} was a structure (matrix, time-series).

This is a description of what we could today call vectorized functions. The exceptional behaviors of the sorting functions are interesting.

References:

John M Chambers, Computational Methods for Data Analysis, 1977

R Gnanadesikan, Methods for Statistical Data Analysis of Multivariate Observations, 1977

K V Mardia, J Kent, J M Bibby, Multivariate Analysis, 1979

\paragraph{Becker, Chambers and Wilks (1988)~\cite{Becker1988}}

The definitive book introducing the statistical programming language S in its
modern form, which was adapted by its open source implementation, R.

p. 125: Classes of objects in S

Class | Use
vector | simple objects
matrix | 2-dimensional array
array | multi-way array

Notable for saying on Ch. 5, p. 128

\begin{quote}
If the matrices resulting from subscripting end up with one row or one
column, there is a chocie to make. S can retain the matrix properties or it can
drop one dimension to rpoduce a vector. By default, S drops these redundant,
or ``dead,'' dimensions. [...]
In most cases of interactive computing, that is the right choice since it tends to
reuduce complexity---why have a 1-column matrix when a vector will do.
\end{quote}

which is not mentioned in the 1984 book.
\todo{Are dropping dimensions mentioned in ``Extending the S System'', 1985?}






\todo{EISPACK is based on Wilkinson and Reinsch, Handbook for Automatic Computation, Vol 2, Linear ALgebra, Pt 2}

\cite{Landin1965}:
\begin{quote}
An array is considered as a function whose domain is a subset of the set of integer-lists.
It is initialized with the appropriate domain (not subsequently altered) and with all of its elements equal.
\end{quote}

\todo{Requested: secondary references in ``History of Fortran i-3'' by Backus}

Moser - A-2 compiler (1954) and one other thing

mention of array o subscript variables?

\todo{TAoCP v3 credits Von Neumann with inventing merge sort. Did he also invent the array?}

Algol 1958 - array variables may be multidimensional, must correspond to subscripted variable with as many empty parameter positions as dimensions.
In other words, the dimensionality of an array in Algol is crucial to determine how many indexes to supply.

PL/I
\begin{quote}
Since it appears to be the
easiest way during interpretation to handle data aggregates by recursively defined
functions or instructions level by level, the structuring of data attributes is
described level by level:

(a)
Arrays. A multi-dimensional array is decomposed into a nested sequence of
one-dimensional arrays; e.g.\ a two-dimensional array of scalars is handled
as a one-dimensional array, whose elements are themselves one-dimensional
arrays of scalars. An array of structures or cells is naturally handled in
the same way with the only difference that its base elements are described
as structures or cells. So, an abstract program has only one-dimensional
array data attributes; the elements may be arrays, structures, cells or
scalars.

Array data attributes consist of three components: An expression denoting
the lower bound (if missing in the concrete program, the constant 1 is inserted
by the translator), an expression denoting the upper bound and the
data attributes of the elements of the array

--- \cite[\S 2.3.1, pp. 2-8--9]{Lucas1968}
\end{quote}

Secondary reference – Knuth,
\url{http://bitsavers.informatik.uni-stuttgart.de/pdf/stanford/cs_techReports/STAN-CS-76-562_EarlyDevelPgmgLang_Aug76.pdf}
'
Plankalkul – Vektoren


``first abstraction language'' - list

\url{http://deepblue.lib.umich.edu/bitstream/handle/2027.42/3966/bab9692.0001.001.pdf?sequence=5&isAllowed=y}

Rutishauser – subscripted variables with calculable offsets

Brooker, 1955: variables with indices vn1...vn5
\url{http://iopscience.iop.org.libproxy.mit.edu/article/10.1088/0508-3443/6/9/302/pdf}

Blum ADES II 1956 – subscripts on variables
\url{http://delivery.acm.org.libproxy.mit.edu/10.1145/810000/808962/p110-blum.pdf?ip=18.9.61.111&id=808962&acc=ACTIVE%20SERVICE&key=7777116298C9657D%2EDE5F786C30E1A3B4%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=574921315&CFTOKEN=61166642&__acm__=1452667089_44d3b1dd2f947c885f8426bdd8048852}
also reference to “vectors”

\url{https://archive.org/stream/bitsavers_ibm650adesDigitalEncodingSystemIIFeb56_4727600/NAVORD_4209_Automatic_Digital_Encoding_System_II_Feb56#page/n23/mode/2up}
“array”, “vector”, “index”

IT 1956: JACKPOT1!!!!!!!!!!!!!!!
\url{http://delivery.acm.org.libproxy.mit.edu/10.1145/810000/808963/p114-chipps.pdf?ip=18.9.61.111&id=808963&acc=ACTIVE%20SERVICE&key=7777116298C9657D%2EDE5F786C30E1A3B4%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=574921315&CFTOKEN=61166642&__acm__=1452667765_f7f48bfe2bce87dbf76b28e0a9f84cf0}
scalars, vectors, matrices  and explicit indexing formulas
x is multiply, * is exponentiation


“indicial instruction machines” - 1959
variable address index
\url{https://archive.org/stream/bitsavers_trwGrabbeRokofAutomationComputationandControlVol21_46695468/Grabbe_Ramo_Wooldridge_Handbook_of_Automation_Computation_and_Control_Vol_2_1959#page/n109/mode/2up}

“matrix array”
\url{https://archive.org/stream/bitsavers_trwGrabbeRokofAutomationComputationandControlVol21_46695468/Grabbe_Ramo_Wooldridge_Handbook_of_Automation_Computation_and_Control_Vol_2_1959#page/n465/mode/2up/search/array}

“the art of programming”
\url{https://archive.org/stream/bitsavers_trwGrabbeRokofAutomationComputationandControlVol21_46695468/Grabbe_Ramo_Wooldridge_Handbook_of_Automation_Computation_and_Control_Vol_2_1959#page/n59/mode/2up}

The B line of Manchester University
\url{https://archive.org/stream/programsforelect00wilk#page/14/mode/2up/search/index}

“column vector”
\url{https://archive.org/stream/bitsavers_trwGrabbeRokofAutomationComputationandControlVol11_37731457/Grabbe_Ramo_Wooldridge_Handbook_of_Automation_Computation_and_Control_Vol_1_1958#page/n259/mode/2up/search/vector}


\paragraph{Apostol (1969)~\cite{Apostol1969}}

By the time Apostol's well-known textbook on calculus was published, we see:

\cite[p. 91]{Apostol1969} defines the matrix transpose elementwise, which is perhaps the more familiar form

The transpose of an $m \times n$ matrix
$A = {\left(a_{ij}\right)}^{m,n}_{i,j=1}$ is the $n \times m$ matrix $A^t$
whose $i, j$ entry is $a_{ji}$.

The adjoint shows up on \cite[p. 122]{Apostol1969}

Apostol uses ``column matrix'' and ``column vector'' synonymously: \cite[p. 592]{Apostol1967} conflates the notions of tuple, array, vector and matrix (!):

We shall display the $m$-tuple $(t_{1k}, \dots, t_{mk})$ vertically... [t]his array is called a \textit{column vector} or a \textit{column matrix}.

If we interchange the rows and columns of a rectangular matrix $A$, the new matrix so obtained is called the \textit{transpose} of $A$ and is denoted by $A^t$. \cite[p. 615, Exercise 7]{Apostol1967}.

(Another axiom: ${(A^t)}^{-1} = {(A^{-1})}^t$)

\paragraph{Array theory} The APL literature contains a comprehensive discussion
of multidimensional array indexing semantics~\cite{Brown1982}, which were
absent in the classical APL of \cite{Iverson1962}. \cite{Ruehr1982,Gerth1988}
surveyed the development of array semantics in APL.

What we refer to as APL style indexing was first written down in the APL/360
manual~\cite{Falkoff1968}, albeit only for rank 2 only, where indexing by a
matrix returned a matrix. \cite{Haegi1976} noted an inconsistency. The general
case has been termed ``scatter-point indexing'' or ``choose indexing''
\cite{Brown1972,Ruehr1982}, and was perhaps most clearly described
in~\cite{More1979}. \cite{Gull1979} emphasized the importance of preserving
the rank of the index set in the rank of the output for multidimensional
arrays.

\cite{More1973} built a theory of multidimensional arrays using recursive
nesting of one-dimensional arrays, formalized upon axiomatic set theory.
\cite{Ghandour1973} emphasized the ordering of elements as a defining
characteristic of arrays. \cite{Gerth1988} emphasized indexing as a function on
arrays from index sets to value sets. We do not consider functional aspects of
indexing in this paper.

The APL literature makes some mention of linear algebra semantics, but does not
consider the interplay with array indexing semantics.

\begin{enumerate}
\item \cite{More1973} mentions the inner product returning a scalar as an
	important rule for guiding array semantics.

\item \cite{Haegi1976} bemoans the conflation of scalars and $(1,)$-arrays in
	classical APL.

\item \cite[p. 153]{More1973} also suggests the possibility of more complicated
	multidimensional array semantics:

\begin{quote}
Let $V$ be an $n$-dimensional vector space over a field. Disregarding
considerations of contravariance and covariance, a tensor of valence $q$ on $V$
is a multilinear mapping of the Cartesian product of the list $V V \dots V$ of
length $q$ into a vector space. If $V$ has a base, then a tensor of valence $q$
on $V$ can be represented by a \textit{component tensor}, which is an array on
$q$ axes, each of length $n$.
\end{quote}

\end{enumerate}


\subsection{OTHER IDENTITIES?}
Another identity ${(Au - v)}^T(Au - v) = u'Au -...$
