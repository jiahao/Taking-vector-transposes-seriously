\label{sec:array}
\section{Axiomatic behavior of arrays}

\subsection{Indexing is subscripting}

How do we index $x^{T}$ and $y$? If $y$ is represented by a 1-D
array then it is indexable by a single subscript, $y[i]=y_{i}$. Indexing
really is a subscript operation - the earliest literature on array
data structures say as much.\footnote{The array is such a fundamental data structure that it is difficult
to pin down a concrete definition. Nevertheless, the early implementations
of arrays in languages such as FORTRAN~\cite[p36]{Backus1956} and
ALGOL~60~\cite[Sec. 2.3]{Randell1964} make clear that indexing
by a fixed number of integers is the most primitive operation associated
with an array.} The earliest mention of array data structures describe then as subscripted
variables. (CAN SAY MORE) It's intuitively pleasing since in the canonical
basis $e_{1},e_{2},\dots,e_{n}$ one writes
\begin{equation}
y=y_{1}e_{1}+y_{2}e_{2}+\dots+y_{n}e_{n}.
\end{equation}
and we can say that the collection of numbers $y_{1},y_{2},\dots,y_{n}$
represents the vector $y$. We see from this discussion that indexing
a numeric vector is equivalent to premultiplying by a particular basis
vector, $e_{i}\cdot y=e_{i}\cdot\left(\sum_{j}y_{j}e_{j}\right)=\sum_{j}y_{j}e_{i}\cdot e_{j}=y_{i}$
since the canonical basis is orthogonal.

$x^{T}$ is different though. Nominally it is a row vector, so if
we write down the analogous relations for $x$ and transpose them,
we get
\begin{equation}
x^{T}=\left(x^{T}\right)_{1}e_{1}^{T}+\left(x^{T}\right)_{2}e_{2}^{T}+\dots+\left(x^{T}\right)_{n}e_{n}
\end{equation}
and $x_{i}^{T}=x^{T}e_{i}=x\cdot e_{i}$. Everything looks normal
for real vectors, but in the complex case we have by property of the
inner product that
\[
\left(x^{\prime}\right)_{i}=x\cdot e_{i}=\left(e_{i}\cdot x\right)^{*}=x_{i}^{*}
\]
and we suddenly now have to remember to complex conjugate. In other
words $x^{\prime}[i]$ should be the complex conjugate of $x[i]$.

Now consider a matrix, which are ordinarily represented by 2D arrays.
A 2D array is defined recursively as an array of arrays. In contrast,
a matrix must be defined either as a row vector of column vectors
or a column vector of row vectors. A matrix cannot be defined as a
column vector of column vectors or a row vector of row vectors. Thus
again we are confronted with the fact that 1D arrays are intrinsically
``flat'', lacking the rowness or columnness that vectors possess.

If we again consider the process of indexing, we have again an equivalence
between indexing an array and the bilinear form with two canonical
basis vectors.
\begin{equation}
A_{ij}=e_{i}^{T}Ae_{j}=A[i,j]
\end{equation}

Whereas for 2D arrays we can recursively index $A$ as $\left(A[i]\right)[j]$,
the matrix analogue requires $\left(e_{i}^{T}A\right)e_{j}$ to be
the composition of two fundamenatally different operations. The first
is a (row) vector–matrix product, returning a row vector, which then
gets multiplied with a column vector to yield the scalar $A_{ij}$.

We see this again in ``baby matmul''. Definining $C=AB$ as the
matrix with elements $C_{ij}=\sum A_{ik}B_{kj}$, we sometimes see
that matmul is defined as saying that $C_{ij}$ , the element of $C$
in the $i$th row and $j$th column, is the product of the $i$th
row of $A$ with the $j$th row of $B$. At least for nonreal matrices,
this product is \uline{not} the same as the inner product $u\cdot v$!
The $i$th row of $A$ is not conjugated, reflecting the fact that
$C$ represents the composition of two linear maps $A$ and $B$.
If we write this operation as $A[i,:]*B[:,j]$, then we need the version
of indexing that does not conjugate the row in $A[i,:]$.

Thus we see that for real numbers the operations of indexing (subscripting)
and taking inner products are deeply intertwined. However for nonreal
numbers we inconsistently switch between the ``array world'', where
everything is intrinsically flat, and ``linear algebra'' world,
where the additional structure of the inner product implies additional
complexity for nonreal matrices and vectors.


\subsection{Vector indexing and inner products}

\paragraph{Indexing is an inner product operation}
Indexing is a key aspect of array semantics. For array data structures, the
most basic form of indexing maps an index to an element of a homogeneous
collection. For numeric data types, the indexing operation \lstinline|v[i]| on
a one-dimension array \lstinline|v| of length $n$ can also be expressed in the
language of linear algebra as an inner product of two vectors
\begin{equation}
	v_i = e_i \cdot v,\label{eq:idx1}
\end{equation}
where $v$ is the array \lstinline|v| interpreted as a vector $(v_1, v_2, \dots,
v_n)$, and $e_i$ is the $i$th canonical basis vector $(0, 0, \dots, 0, 1, 0,
\dots, 0)$ where the $i$th entry is nonzero.%
\footnote{In this paper, we assume column major order and one-based indexing,
which is prevalent in the numerical computing literature and also in
programming languages which are popular for numerical computing such as
FORTRAN, MATLAB and Julia. The discussion in this paper apply equally to row
major order and/or zero-based indexes with only minor notational changes. Also,
we use the Householder notation convention that is ubiquitous in the numerical
linear algebra literature, where vectors are named with small Latin letters,
matrices are named with capital Latin letters, and scalars not associated with
the entry of any vector or matrix are named with small Greek
letters.~\cite{Householder1964}}
The preceding discussion requires the following to hold:

%\begin{enumerate}[label=\bfseries{}A\arabic*]
%\item
The equivalence between indexing and inner products holds for
		any data type for which the integer 1 is a multiplicative
		identity.
%\label{point1}
%\end{enumerate}
%
%\todo{unital algebra}
%
%\todo{indexing is more fundamental than linalg. You can index vectors of strings, for example.}


\paragraph{Slicing is a vector-matrix product}
Many languages for technical computing like FORTRAN, APL, MATLAB, Python/NumPy,
and Julia also feature more sophisticated indexing semantics, such as slicing.
Slicing takes an array of indexes rather than a single scalar index, and returns
another array containing the respective elements. For example, a Julia
expression like \lstinline|v[1:3]| returns a new vector with entries
\lstinline|v[1], v[2], v[3]|.

Now consider the analogous argument that a slicing operation can also be
expressed in the language of linear algebra: \lstinline|v[1:3]| corresponds to
the matrix-vector product
\begin{equation}
	\begin{pmatrix}e_1 & e_2 & e_3\end{pmatrix}^\prime v,\label{eq:idx2}
\end{equation}
where $\begin{pmatrix}e_1 & e_2 & e_3\end{pmatrix}$ is the $n \times 3$
matrix with columns $e_1$, $e_2$ and $e_3$ respectively and $'$ denotes matrix
transposition.%
\footnote{For simplicity, we consider only real numeric types, although the
discussion holds also for other algebraic fields such as complex numbers when
complex conjugations are combined appropriately with transposition.}
The equivalence follows from working out the product row by row,
and noting that
\begin{equation}
	e_i^\prime v = e_i \cdot v = v_i,\label{eq:v'}
\end{equation}
where the last equality follows from \eqref{eq:idx1}.

The astute reader would have noted that in going from \eqref{eq:idx2} to
\eqref{eq:v'}, we sneaked in the notion of a \textit{vector transpose}, having
extracted rows of a matrix for subcomputations of the form \eqref{eq:v'}.
Whereas $(e_1)$ is an $n \times 1$ matrix, $e_1$ is an $(n,)$-vector. This
distinction is glossed over by numerical analysts, but the conflation of these
two objects poses a problem for computer scientists: ${(e_1)}^\prime$ must be
a $1 \times n$ matrix from the rules of matrix transposition, but what then is
$e_1^\prime$? Recall that the transpose of an $m \times n$ matrix $A$ is an
$n \times m$ matrix $A^\prime$ whose element in the $i$th row and $j$th column
is
\begin{equation}
	{\left(A^\prime\right)}_{ij} = A_{ji}.\label{eq:A'}
\end{equation}
Transposition clearly swaps the two indexes. But what should transposition do
for an $(n,)$-vector, which has only one dimension?

The semantic problem posed by vector transposition arose from the assertion
that the columns of a matrix are column vectors. One could argue that an
expression of the form $\begin{pmatrix}v_1 & v_2 & \dots v_k \end{pmatrix}$
should not be thought of as the concatenation of vectors, but rather $n\times1$
matrices. Thus one sees here the intermixing of indexing semantics and
algebraic semantics, since one now has two choices of indexing semantics:

%begin{enumerate}[label=\bfseries{}C\arabic*]
%\item
A column slice \lstinline|A[:, i]| returns an $(n,)$-array.\label{rule:col1}
%\item
A column slice \lstinline|A[:, i]| returns an $(n,1)$-array.\label{rule:col2}
%\end{enumerate}

Bearing these two choices in mind, we can now list possible choices for the
meaning of the expression \lstinline|v^\prime|. Let \lstinline|v| be a $(n,)$-array. Then:

%\begin{enumerate}[label=\bfseries{}T\arabic*]
%\item
Transposing a vector, \lstinline|v^\prime|, returns a $(n,)$-array, which is just
      $v$.\label{rule:v'1}
%\item
Transposing a vector, \lstinline|v^\prime|, returns a $(1,n)$-array.\label{rule:v'2}
%\item
Transposing a vector, \lstinline|v^\prime|, returns an object of shape $(n,)$,
      which is in a dual space of some sort. \textit{not} a conventional array
      that lies outside our current system.\label{rule:v'3}
%\end{enumerate}
%
\todo{T2 requires a cast of v vector into (v,) matrix, then you transpose it.}

\ref{rule:v'2} follows naturally from \ref{rule:col2} and the definition of
matrix transposition in \eqref{eq:A'}, which swaps the indexes. Similarly,
\ref{rule:v'1} follows naturally from \ref{rule:col1} and noting that
``swapping'' the order of indexes is a no-op, since the identity is the only
permutation of length 1. \ref{rule:v'3} is perhaps the least intuitive, but
can be justified by noting that \lstinline|v^\prime| is a row vector, which is not
identical to a column vector and therefore cannot have the same representation.
A less conventional justification of \ref{rule:v'3} is to regard \lstinline|v^\prime|
not as an array, but a dot product expression which is curried in the first
argument. In other words, \lstinline|v^\prime| is an incomplete expression waiting to
be multiplied with another $(n,)$-vector \lstinline|w| so that
%
\begin{equation}
	\alpha = v^\prime w = v \cdot w,\label{eq:w'v}
\end{equation}
%
%TODO clarify the point is that we want equality between the rules of matrix
%multiply (lhs) of second equality, and the rhs is vector dot product.
%Remind people tht (mxn) x n => m.
%Make the point that T1, T3 it is natural to take this equation as the definition as
%of the product whereas T2 has to already obey matmul.
%
%TODO v*w actually means matmul. But Alan wants to consider the case where we overload * to mean scalar*non-scalar also as a special case.


This interpretation of \lstinline|v^\prime| corresponds closely to the notion of a
linear functional, which is used in functional analysis.

We see immediately that while \ref{rule:v'1} and \ref{rule:v'3} satisfy
\eqref{eq:w'v}, producing a scalar result, \ref{rule:v'2} results in a product
involving quantities of shapes $(1,n)$ and $(n,)$, which by the usual rules of
linear algebra must produce a quantity of shape $(1,)$ and not a scalar. Some
programming languages like MATLAB employ an additional rule to circumvent this
problem, which is to define scalars and $(1,)$-vectors to be equivalent for the
purposes of equality comparison. More generally, we have the rule

%\begin{enumerate}[label=\bfseries{}I\arabic*]
%\item
An array $A$ of shape $(n_1, n_2, \dots, n_k)$ defines an equivalence
class which also includes reshapings of $A$ into arrays of shape $(n_1, n_2,
\dots, n_k, 1)$, $(n_1, n_2, \dots, n_k, 1, 1)$, and other shapes with an
arbitrary number of trailing singleton dimensions, all of which contain the
same values. For $n = k = 1$, scalar quantities are also included in this
equivalence class.\label{rule:trail1}
%\end{enumerate}

The equivalences defined by \ref{rule:trail1} remove some of the semantic
distinctions made earlier. First, the result of column indexing is no longer
ambiguous, as the combination \ref{rule:trail1}+\ref{rule:col1} is equivalent
to \ref{rule:trail1}+\ref{rule:col2}. Second, the objection to \ref{rule:v'2}
on the grounds of the inner product is mollified, as each of
\ref{rule:v'1}, \ref{rule:trail1}+\ref{rule:v'2} and \ref{rule:v'3} define
correct inner products \eqref{eq:w'v}.



\subsection{Outer products}

Inner products are not the only kind of vector-vector product in linear
algebra. There is also the outer product $vu'$, which for a $(n,)$-vector $v$
and a $(m,)$-vector $u$ is the $n \times m$ matrix whose element in the $i$th
row and $j$th column is $v_i w_j$. One can think of the outer product as a
particular kind of matrix-matrix product, generated from the product of a $n
\times 1$ matrix and a $1 \times m$ matrix.

It turns out that the outer product cannot be written as \lstinline|v*u'| with
\ref{rule:v'1} semantics, since \lstinline|vu'| is the product of a $(n,)$
vector with a $(m,)$ vector. This product is not defined except for $n = m$,
whence \lstinline|v*u'| evaluates to the inner product! This result follows
from \ref{rule:v'1} semantics and \eqref{eq:w'v}, since $vu^\prime = vu =
v^\prime u = v \cdot u$.

\ref{rule:v'2} is also problematic, as it yields the product $(n,) \times (1,
m)$, which is incommensurate. Only the combination
\ref{rule:trail1}+\ref{rule:v'2} allows this product to be defined sensibly,
since \ref{rule:trail1} allows $(n,)$ to be reshaped to $(n,1)$, and then the
product can be evaluated using the ordinary rules of matrix-matrix
multiplication.

\ref{rule:v'3} semantics give us the flexibility of defining the outer product
axiomatically as the product \lstinline|u*v^\prime|, since $v^\prime$ is represented
by a different type.



\subsection{Bilinear forms and the trace operation}

In this section we consider bilinear forms, which are vector-matrix-vector
products of the form $v'Aw$ for an $n$-vector $v$, $n\times m$ matrix $A$, and
$m$-vector $w$ and whose result is a scalar:
%
\begin{equation}
	\beta = \sum_{i=1}^n \sum_{j=1}^m v_i A_{ij} w_j.\label{eq:v'Aw}
\end{equation}

By definition, a bilinear form can also be written using the trace operator.
The trace of a $n \times n$ matrix $A$ is defined as the sum of its diagonal
entries, i.e.
%
\begin{equation}
	\gamma = \mathrm{tr }A = \sum_{i=1}^n A_{ii}.\label{eq:tr}
\end{equation}

The bilinear form therefore must satisfy the identity
%
\begin{equation}
	\beta = v^\prime Aw = \mathrm{tr }Awv^\prime.\label{eq:trid}
\end{equation}

For this identity to be expressible in code of the form
\begin{lstlisting}
v'*A*w == tr(A*w*v')
\end{lstlisting}
, we need the semantics for both inner products and outer products.
Thus the only minimal sets of semantics that obey this identity are
\ref{rule:trail1}+\ref{rule:v'2} or \ref{rule:v'3}.

%TODO say that for the special case of A=I then you can satisfy (5) and be happy.


\subsection{Invalid operations}

Equally important to the story of capturing algebraic semantics is the story of
capturing \textit{invalid} semantics. Examples of invalid products are

\begin{enumerate}
\item Vector-vector products $vw$,
\item Transposed vector-transposed vector products $v^\prime w^\prime$,
\item Vector-matrix products $vA$, and
\item Matrix-transposed vector products $Av^\prime$.
\end{enumerate}

All these expressions are valid with \ref{rule:v'1} semantics, since any
vector-vector product is the inner product regardless of transposition,
$vA = v^\prime A$, and $Av^\prime = Av$.

\ref{rule:v'2} semantics sometimes yield valid expressions for these products:
%
\begin{enumerate}
\item When $v$ and $w$ are each $1$-vectors, $v^\prime w^\prime$ is a matrix
product yielding a $1\times 1$ matrix.
\item When $A$ is a $n\times 1$ matrix and $v$ is an arbitrary vector (say of
length $m$), $Av^\prime$ yields a $n\times m$ matrix.
\end{enumerate}
%
In this case, considering also \ref{rule:trail1} semantics
exacerbates the problem by creating \textit{more} valid cases:
%
\begin{enumerate}
\item When $v$ and $w$ are each $1$-vectors, $vw$ can be interpreted as a
matrix product of two $1\times1$ matrices, yielding a $1\times1$ matrix.
\item When $A$ is a $1\times n$ matrix and $v$ is an arbitrary vector of length
$m$, then $vA$ produces a $m\times n$ matrix.
\end{enumerate}
%
Thus one could say that \ref{rule:trail1}+\ref{rule:v'2} enables the expression
of inner products, outer products and bilinear forms, but at the price of
falsely allowing some invalid products as valid expressions.

Only \ref{rule:v'3} semantics can be shown to reject all these expressions as
invalid products. With the flexibility afforded by considering a transposed
vector as a different type, we can simply define $v^\prime w^\prime$ and
$Av^\prime$ to be erroneous. The other two products $vw$ and $vA$ are already
errors by default, unless \ref{rule:trail1} is also active, in which case $vw$
and $vA$ can be erroneously valid also.

We therefore conclude that \ref{rule:v'3} is the only valid semantic rule that
correctly excludes all erroneous products.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN CRUFT

Vectors are also defined
by taking rows and columns of matrices. A typical linear algebra text
introduces notation for columns of a matrix by rewriting a matrix $A$ as
$A = [a_1, a_2, \dots, a_n]$ or even~\cite[for example,
p. 6]{Trefethen1997}
\[
A = \left[\left.\begin{array}{c}
\\
\\
a_1\\
\\
\\
\end{array}\right|\begin{array}{c}
\\
\\
a_2\\
\\
\\
\end{array}\left|\begin{array}{c}
\\
\\
\ \cdots\ \\
\\
\\
\end{array}\right|\begin{array}{c}
\\
\\
a_n\\
\\
\\
\end{array}\right].\label{eq:cols}
\]
%
$a_k$ is therefore a vector that is the $k$th column of $A$.  Thus we can see that
the indexing semantics are close in spirit to the operation of extracting
submatrices from matrices. However, it is less clear from the definitions if
the indexing semantics and algebraic semantics are fully compatible.



\subsection{Matrix slices and column vectors}

Matrices have a natural representation in 2-arrays:

%\begin{enumerate}[label=\bfseries{}M\arabic*]
%\item
An $m \times n$ matrix $A$ with $m$ rows and $n$ columns can be
	represented by a $(m,n)$-array, i.e.\ an array of rank 2, size $m$ in
	the first dimension, and size $n$ in the second
	dimension.\label{rule:mat}
%\end{enumerate}


%Such slicing notation was originally developed for array indexing, but has
%since found its way into the mathematical literature, particularly in applied
%mathematics, where notation like $V(:, 1)$~\cite[p.
%2]{Higham2002},\cite[p. 38]{Strang2003} can be found in mathematical formulae.
%Similarly, slice indexing semantics are convenient for defining subblocks of
%matrices, such as
%\begin{lstlisting}
%B = A[1:p, 1:q]
%\end{lstlisting}
%which extracts the first $p$ rows and first $q$ columns of a matrix $A$ into a
%new matrix $B$.

\subsection{Generalizations to higher rank}

\ref{rule:col1} is a special case of the indexing rule where the rank of the
result is the sum of the ranks of the indexes. This indexing rule is used by
some languages such as APL.\@ The history of this rule is discussed in
related work.

It is tempting to generalize \ref{rule:v'1} to arrays of general rank, by
considering $A^\prime$ as the array constructed by reversing all the indexes of
$A$. While by itself a valid rule, it is fundamentally incompatible with
\ref{rule:trail1} semantics. Applying the ``reverse all indexes'' rule to the
equivalence class defined in \ref{rule:trail1} produces an equivalence class
comprising arrays of shapes $(n_k, n_{k-1}, \dots, n_1)$, $(1, n_k, n_{k-1},
\dots, n_1)$, $(1, 1, n_k, n_{k-1}, \dots, n_1)$, and so on. In other words,
the resulting equivalence class under transposition defines an arbitrary number
of \textit{leading} singleton dimensions, with the result that the first
dimension of the array is no longer uniquely defined. We therefore reject the
combination of \ref{rule:trail1}+\ref{rule:v'1} semantics as mutually
inconsistent.



\subsection{Data structures for sparse matrices}

This discussion may seem a little silly, but one should recognize
that the situation for sparse matrices is a confusing hodgepodge of
both kinds of semantics. The so-called compressed sparse column or
compressed sparse row representations can only support A1 semantics.
These data structures are analogous to representations of $\left(m,n\right)$-tries.
In contrast, the so-called coordinate or IJV format naturally generalizes
to any number of dimensions.

\subsection{Implementations in current languages}


...

Inconsistent design choices can cause user frustration. For example,
consider the Rayleigh quotient, denoted $v^{\prime}Av/v^{\prime}v$.
If $v^{\prime}v$ produced a scalar, the quotient can produce a scalar.
Similarly, if $v^{\prime}v$ produced a $\left(1,1\right)$-array,
we can interpret the result as a $1\times1$ matrix and interpret
the division $/$ as a matrix-matrix right division, i.e.\ the operation
$A/B=AB^{-1}$. However, if $v^{\prime}v$ produced a $\left(1,\right)$-array,
the usual correspondence to a 1-vector causes this operation to be
undefined, since right division by a vector is undefined. A $(1,)$-array
can be formed if $v^{\prime}$ produced a $1\times N$ matrix, which
when postmultiplied by a $N$-vector must produce a $1$-vector by
the rules of linear algebra. One could special case the length 1 case
but this feels unnatural. Yet another possibility is to define \textbf{implicit
trailing dimensions}, which is a type conversion rule that (in this
context) says that a $(1,)$-array can behave as if it is a $(1,1)$-array
when convenient. With implicit trailing dimensions, right division
by $v^{\prime}v$ can be defined by converting the $(1,)$-array into
a $(1,1)$-array and using the matrix right-division rule. Note that
implicit trailing dimensions allows the data structures of A2 to adopt
the semantics of A1.

%END CRUFT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Matt Bauman's thoughts}

There are a number of paths towards a consistent set of semantics that link
array container behaviors with linear algebra. The most visible aspects of
these behaviors are indexing and the vector transpose.


There are three choices for what `v'` does when `v` is a vector with shape `(n,)`:

A1. Do nothing (`v'==v`) or error right away
A2. Return a `(1,n)` matrix
A3. Return a special `n`-element type that acts like a one-dimensional row vector (the covector)


There are also three choices for indexing behaviors in terms of dropping
scalars (e.g., given `A=rand(2,3,4)`, what does `A[2,:,1]` return?):

B1. Drop all scalar dimensions. `A[2,:,1]` is a `3`-element 1-dimensional thing
B2. Drop trailing scalar dimensions. `A[2,:,1]` is a matrix with shape `(1,3)`
B3. Never drop any scalar dimensions. `A[2,:,1]` is an array with shape `(1,3,1)`

APL chooses A1 and B1. The transpose operator `⍉` is defined as a permutedims,
reversing the order of all dimensions; as such, it has no effect on scalars or
vectors. This means that the inner product isn't defined in terms of
transposes... instead it's typically formulated in terms of a map-reduce style
operation along the last and first axes (`+.×`), which is also the definition
of matrix multiplication [microapl doc; notice how they define this in terms of
indexed slices][1]. Similarly, the outer product (`∘.×`) is map-style operation
defined in terms of elements in arrays [microapl doc][2], with the result being
an array whose shape is the shape of the left argument followed by the shape of
the right argument. More generally, reshaping (`ρ`) seems to be a fairly common
operation, but I've never used APL myself so corrections are welcome. Taken
together, this forms a relatively consistent universe, based upon container
semantics. Additionally, there is a separate conjugation operator (I believe
it's simply `+`), that must manually be applied to obtain the conjugate
transpose (`+⍉`), so complex dot products are tricky -- you must remember to
manually conjugate the left-hand argument.

Matlab chooses A2 (well, in a way...) and B2. Matlab makes this universe
consistent by making *everything* a matrix. ``Scalars'' are simply `(1,1)`
matrices. ``Vectors'' are `(1,n)` or `(n,1)` matrices [Mathworks doc: Empty
Matrices, Scalars, and Vectors][3]. In general, there is an equivalence class
between any structure that has the same number of trailing ones. As such, there
really is no such thing as a vector with shape `(n,)`, so they don't really
follow rule A2 -- they don't need to worry about it at all. What this means,
though, is that there are special cases for matrices with a singleton dimension
that don't exist in classical linear algebra. For example, the product between
a `(m,n)` matrix (with nonzero `m` and `n`) with a `(1,1)` matrix should be
undefined. But in Matlab that's a scalar multiplication. Since leading
dimensions are preserved, the row slice `A(1,:)` is still a row. Thus matrix
multiplication can be expressed as the inner product of slices. And ``vector''
outer products `vv'` are really just matrix multiplications. Interestingly,
Matlab tries *very* hard to preserve the orientation of vectors, at the expense
of another special case in its indexing rules. This means that a row-vector
obtained by the slicing operation `A(1,:)` will remain a row-vector, even after
indexing it with a column vector. The only exception is that indexing with the
`:` operator will indeed return a column vector. Similarly, column vectors are
preserved in the operation `v(1:end)`, even though the range `1:end` is a row
vector. I find it fascinating that the special case for row vectors is a direct
consequence of supporting *linear indexing*, which would otherwise effectively
allow you to drop the leading unity dimensions. Without support for linear
indexing, this is an illegal operation.

So that brings us to Julia. We currently use A2 and B2, but we haven't added
all the special cases that Matlab uses in order to make this work as
consistently as it does in Matlab: `5 ≠ [5] ≠ [5]''`. This means that there are
valid mathematical statements that don't quite work in Julia, even though they
are unambiguous in mathematical literature and work in Matlab. For example,
`v * v' * w` is not associative with `v` and `w` vectors of the same length.
Scaling matrices by the dot product (or inverse dot product) of a vector only
works when you spell the dot product as `dot` and not `v'v`. We allow
multiplication between `(n,) × (1,m)` (implicitly treating the vector as a
`(n,1)` matrix), but the result of `(m,n) × (n,)` is `(m,)` (the vector dropped
a dimension!). We'd like to fix these issues.

One proposed solution is a special 1-dimensional covector type that acts like a
transposed vector. Whereas vectors would only allow being multiplied by a
matrix from the left (and drop a dimension), covectors would only allow
multiplication with a matrix on the right. `(m,n) × (n,)` is an `(m,)` vector,
but `(,m) × (m,n)` is a `(,n)` covector (please pardon the abuse of tuple
notation as a way to express the row-ness of the covector). The inner product
`v'v` is `(,n) × (n,)`, and returns a scalar, whereas `v v'` is `(n,) × (,n)`
and returns an `(n,n)` matrix. Adding an extra type here does add complexity,
but I think it could be presented in a way that most users could grasp fairly
easily without delving into dual spaces and down-up matrices, etc (but maybe
I'm naive).

At the same time, we'd really like to change our indexing behaviors. APL-style
indexing is a more expressive superset of our current behavior. You can
manually retain indices by using a 1-element UnitRange. And I'd really like to
add the capability to add dimensions by indexing by higher rank indices [see footnote]. Plus,
``the rank of the result is the sum of the rank of the indices'' is catchy and
simple.

The tricky part about APL indexing is that it loses the orientation of row
vectors, and so we can no longer define complex matrix multiplication in terms
of slices directly:

* `A[1,:] * B[:,1]` is an illegal `(n,) × (n,)` operation.

* `dot(A[1,:], B[:,1])` and `A[1,:]' * B[:,1]` erroneously conjugate the row.

In general, working with complex row slices becomes dicey. When do you
conjugate? Or (c)transpose? A solution here would be to explicitly request that
the shapes of the slices be maintained in two dimensions: `A[1:1,:] * B[:,1:1]`
if you want to use it as a linear algebra construct.

We could have row slices return covectors, but that doesn't generalize well to
ND arrays (what's `A[1,1,:]`? is `A[1,:,:]` a down-up matrix?). While
reductions currently behave differently from indexing (they act like rule B3,
retaining the rank of the source array), we could take this opportunity to make
them drop the reduction dimension, too. This would be type stable, but only if
reductions over rows don't return covectors. Even if we don't change the
behavior of reductions, I believe that this is more evidence that row slices
should return normal vectors.

So, if slices of matrices will always result in vectors, I think that making
vector transpose an error would be a tough sell... simply because we would be
giving back returning vectors all the time with slices! An interesting
middle-ground solution might be to only support the non-complex container
transpose `.'` on vectors, since the need to conjugate is ambiguous and depends
upon the context. It would return a special `RowVector` type that behaves like
the covector above but without conjugation. I don't particularly like this,
however, since it awkwardly straddles the two worlds of dual space linear
algebra (like covectors, but without support for conjugation) and containers
(but it's still just one dimension).

\subsection{Review of options based upon the indexing choice}

In short, I believe that dropping singleton dimensions (option B1) is
inherently a ``container'' operation that is not directly compatible with linear
algebra. Even if we also introduce covectors, I cannot see a way to generalize
slicing behaviors that would rectify this. Folks who want to use Julia's arrays
as linear algebra objects simply must be aware of the discrepancy between
containers and linear algebra.

I do not think that only dropping the trailing scalar dimensions matches well
with Julia's type system. As long as dimensionality is a part of the array's
type, Julia will never be able to go ``full Matlab'' and treat one-element
matrices as scalars in a type-stable manner. As such, we'll never be able to
make this semantic fully consistent.

Always retaining the dimensionality of the source array (option B3) is an
interesting possibility that hasn't been well explored in other systems (as far
as I'm aware). Just as wrapping scalars in a one-element vector or UnitRange
allows option B1 to retain dimensions on a per-axis basis, we could introduce a
new type that explicitly drops dimensions. This still leaves something to be
desired, however, since it requires a new `DropDimension` type that is hard to
explain. It wouldn't be considered a scalar nor an array, but rather a
``special'' type that has its own special-cased semantics. That said, this option
has the strong advantage that it preserves slices of matrices as one-row or
one-column matrices. This means that no slices ever become vectors. If we wish
to make the vector transpose an error, I think that this would be the correct
default. Unfortunately, I think that in general use this will be very awkward.

\subsection{Conclusion}

My preference here is to drop all scalar dimensions, and introduce a covector
type so we are able to holistically define a linear algebra with vectors and
covectors.



[1]: \url{http://microapl.com/apl_help/ch_020_020_880.htm}
[2]: \url{http://microapl.com/apl_help/ch_020_020_890.htm}
[3]: \url{http://www.mathworks.com/help/matlab/math/empty-matrices-scalars-and-vectors.html}
[4]: \url{http://www.mathworks.com/help/matlab/math/matrix-indexing.html#f1-85553}

Footnote: There are two reasons why I like supporting multidimensional
indices. First, it's a generalization of the currently supported 1-index
behavior, which is also shared with Matlab. Secondly, it's a very powerful and
efficient way to express a common operation in processing biosignals: repeated
windowing. For example, neuron spike detection is done by identifying threshold
crossings in a recording of the extracellular potential, and then the spike
waveform is extracted by retrieving a small subset of indices about the
threshold crossing. This can be expressed with `data[(-10:20) .+ idxs']`, where
`data` is the evenly sampled vector of the extracellular potential and `idxs`
are the vector of the threshold crossings. This is possible today, but only
with single vectors of data; generally my data are multichannel and I would
like to operate along all channels at once or select a subset. See
[AxisArrays.jl](https://github.com/mbauman/AxisArrays.jl) for an example where
combining this behavior with axis information makes for very expressive code.



\section{My old notes}

So far we have three distinct proposals for what $'$ means on a vector $v$:

Proposal A: $'$ as identity. $v' == v$
Proposal B: $'$ as matrix transpose. $v'$ promotes $v$ to a Matrix and computes its matrix transpose (our current behavior)
Proposal C: $'$ as dual (adjoint). $v'$ produces a new type Covector

In all three proposals, $M'$ on a matrix $M$ is what we understand by matrix transpose.

I support the last proposal.

The difference between the various meanings of $*$ become clear when considering abstract coordinate-free operations, where vectors are simply elements of a vector space and matrices are linear maps between elements of a vector space.

We then have:

- The matmul \verb`A*B` corresponds to the composition of the map $A : U \rightarrow V$ with the map $B : W \rightarrow Z$. The result is
  \verb`A*B` : $U \rightarrow Im(V) \subseteq Z$ if $V \subseteq W$
- The matvec \verb`A*u` corresponds to the application of the map $A : U \rightarrow V$ to the vector $u \in U$. The result is some $v \in V$.
- The inner product \verb`u'*v` corresponds to the application of the map $u' : V \rightarrow F$. We say that $u' \in U*$, the space dual to $U$.
- The outer product \verb`u*v'` corresponds to constructing a map $u*v' : V \rightarrow U$.
- The scaling \verb`c*A == A*c` corresponds to constructing a new map $c*A : U \rightarrow V$.
- The scaling \verb`c*u == u*c` corresponds to constructing a new element of $U$.
- The matrix transpose \verb`A'` corresponds to the adjoint map $A' : V \rightarrow U$ with respect to an inner product $< , >`$defined by
  \verb`<u, A*v> === <A'*u, v>` for all \verb`u` ∈ U and \verb`v` ∈ V.



Vector in Julia = Vector  + basis

Adjoint vs transpose

Adjoint is ...

Transpose is picking two indices and switching them.

For rank 1:
- adjoint makes sense - produces covector which is a linear functional.
- Transposition is an error - there are not two indices to swap.

For rank 2
- adjoint makes sense - produces comatrix which maps covectors to covectors
- transposition is unique

For rank n:
- Adjoint generalizes beyond rank 2 uniquely
- Transposition is not unique - there are (n choose 2) possible transpositions

Covector = dual wrt to some dot product

Interface of covector
should only support
*(Covector, Vector) = dot product
*(Covector, Matrix) = Covector
'(Covector) = vector (involution v'' == v)
vec(Covector) = vector

You can index into Covectors using (v')[1] = dot(v', e1) but in general this
may NOT be a cheap operation if the covector is not defined wrt to the
``ordinary'' inner product, certainly more expensive than O(1). Hence


Shape of covector?
Stefan's question about shape is similar to questions about shape of Givens rotations?
Normal equations use case


If A is not square


\begin{verbatim}
|   |  Current  | Proposed   |  MATLAB |
|---|---|---|---|---|
| A * A[1,:]   | No    | Yes  |  No  |
| A * A[1,:] '  | Yes   | No  |  Yes  |
|  A[:,1] *A  |   No |   No  | No|
|  A[:,1] '*A  | Yes  | Yes  | Yes  |
\end{verbatim}


and if A is square


\begin{verbatim}
|   |  Current  | Proposed   |  MATLAB |
|---|---|---|---|---|
| A * A[:,1]   | Yes   | Yes  |  Yes  |
| A * A[:,1] '  | No   | No |  No  |
|  A[1,:] *A  |   Yes |   No  | Yes|
|  A[1,:] '*A  | No  | Yes  | No |
\end{verbatim}

My biggest concern is that

(take a row from a 2d array) * (2d array) * (take a column from a 2d array)

which is a common operation still won't work unless we take
(take a row) with covector or perhaps better yet
tag it with a general slot index.

What dimensions to drop in indexing? ``APL style'' seems uncontroversial.
What does vector' give?


\subsection{Summary}

The discussion of column and row indexing rules reveals a fundamental tension
between arrays and matrices which we believe have not been given due
consideration in the literature nor in the design of practical programming
languages for scientific applications.
