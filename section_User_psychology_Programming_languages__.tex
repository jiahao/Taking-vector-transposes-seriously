\section{User psychology}

Programming languages designed for scientific computing have many users which are familiar with linear algebraic constructs expressible using various products and quotients involving matrix and vector quantities. Some of these quantities include:

\begin{itemize}
  \item Inner products, $u^\prime v$
  \item Outer products, $u v^\prime$
  \item Bilinear forms, $u^\prime A v$
  \item Quadratic/Hermitian forms, $v^\prime A v$
\end{itemize}

The notation for these quantities were systematized by Householder \cite{Householder1953,Householder1955}, who used capital Roman letters for matrices, small Roman letters for (column) vectors, and small Greek letters for scalars. Importantly, Householder also uses the vector transpose notation $u^\prime$ to express row vectors, which are then used to construct the above products and forms. \cite{Householder1955} uses $u^T = (u_1, \cdots, u_n)^T$ to express row vectors as tuples, a construct which we shall see later as having ample precedent. \cite{Householder1955} writes the inner product as $u^* v$, noting that $*$ ``denotes the conjugate transpose''. \cite[Sec. 4.01]{Householder1953} introduces the Hermitian form . \cite[Sec. 2.04]{Householder1953} introduces the row vector $u^T$ corresponding to the column vector $u$. Householder also acknowledges in \cite[Sec. 2.04]{Householder1953} the analogy between a numerical vector (as a column of a matrix) and a geometric vector in the sense of Gibbs. He writes the vector transpose as if it follows immediately from the matrix transpose:

If each column of a matrix $M$ is written as a row, the order remaining the same, the resulting matrix is known as the transpose of the original and is designated $M^T$. In particular, if $x$ is the column vector of the $\xi_i$, $x^T$ is the row vector of the $\xi_i$.

The outer product shows up in \cite[Sec. 2.24]{Householder1953}, although Householder does not give the quantity $u v^T$ a special name \footnote{while \cite[Sec. 2.03]{Householder1953} introduces ``outer products'' $[u v]$ , these quantities are known today as bivectors and are conventionally denoted $u \wedge v$.}, and the discussion in context clearly implies that Householder considers vector-scalar-vector products $u \sigma v^T$ special cases of matrix products $U S V^T$.

Interstingly, \cite{Householder1953} does not use the terms ``bilinear form'', ``quadratic form'', or ``Hermitian form''. He uses ``scalar product''. \cite{Householder1955} clearly spells out his notational convention. (\cite{Householder1953} has a missing page which might also spell it out, but it's not clear.)



\section{A timeline of the development of key ideas}

A rough sketch.
Several noteworthy secondary sources:~\cite{Parshall1985,Kleiner2007,Higham2009}

\paragraph{Gauss1795}

Bilinear and quadratic forms go back to Gauss, 1795-1801

\paragraph{Möbius (1827), Calcolo geometrico}

May have introduced the scalar product

\paragraph{Grassmann (1844) - Ausdehnungslehre I~\cite{Grassmann1844,Grassmann1995}}

Vectors were called ``extensive magnitudes''. Relative vectors were called ``displacements'' (,,Strecke``) (\S 14) and were distinguished from absolute vectors, being coordinates of points.

Systematic convention: Capital Roman letters were names of points (absolute vectors). Small Roman letters were names of displacements (relative vectors).

Ausdehnungslehre I introduced the ``open product'' (,,offne Produkt``) (\S 172) as the quantity
\[
S = [A_1().B_1 + A_2().B_2 + \dots]
\]

or in modern notation, what we recognize today as the expansion of a matrix $S$ in rank-1 outer products

\[
S = \sum_i b_i a_i^T
\]

A single term, written $[A().B]$ in Grassmann's notation, is a ,,Produkt mit ein Lücke`` (product with one opening), which is generalized to multiple openings in Ausdehnungslehre II. Such a single term is what we would today call an outer product of two vectors, or a rank 1 matrix.

Ausdehnungslehre I introduces the concept of outer multiplication (,,äussere Multiplikation``)~\cite[\S 34, p.57]{Grassmann1844}\cite[p. 81]{Grassmann1995} as what we would today call the exterior product or wedge product. The term ,,äusseres Produkt``\cite[\S 36, p. 60]{Grassmann1844} (``outer product''\cite[p. 84]{Grassmann1995}) also appears.

\paragraph{Grassmann (1847)~\cite{Grassmann1847,Grassmann1995} - ,,Geometrische Analyse``}

\cite[\S 7, p. 334]{Grassmann1995} introduces the inner product $a \times b$, and the inner square $a^2 = a \times a$.

\paragraph{Hamilton (1847)~\cite{Hamilton1847} - ``On Symbolical Geometry''}

Introduces the term ``scalar product'' as part of the quaternion product. \S. 12 introduces the term ``scalar product'' for the case of parallel vectors (which differs from modern usage by a negative sign). He does not appear to have introduced the general case of non-parallel vectors.

\paragraph{Cauchy (1847) - analytical mechanics}

set of $n$ variables is an ``analytic product''

\paragraph{Cauchy (1853)~\cite{Cauchy1853}.} clefs algébriques??? influenced Grassmann 1862 for notation in the inner product???

\paragraph{Cayley (1858)~\cite{Cayley1858}} - invented matrix product and matrix transpose.
Notation for matrix:
\begin{verbatim}
( a, b, c)
| d, e, f|
| g, h, i|
\end{verbatim}

matvec product $(a, b, c)\!\!(d, e, f)$


\paragraph{Grassmann (1862)~\cite{Grassmann1862,Grassmann2000} - Ausdehnungslehre II.}

Writes inner product as $[A | B]$, mentions $[A \times B]$ as an alternative notation (\S 137, p93 English).

The outer (tensor) product is introduced without a name (Remark after \S 51)

\begin{quote}
,,Ein Produkt, in welchern die Faktoren $a, b, \cdots$ irgend wie enthalten find, werde ich[...] mit $P_{a,b,...}$ bezeichnen`` \cite[p. 24, \S 43]{Grassmann1862}

``A product in which the factors a, b, ... are included in any way I will[...] denote by $P_{a,b,...}$''~\cite[p. 22, \S 43]{Grassmann2000}
\end{quote}

Later, the same quantity is referred to informally as
,,ein beliebiges Produkt $P_{a_1, a_2, ..., a_n}$``~\cite[\S 353]{Grassmann1862} (``an arbitrary product''~\cite[p. 196, \S 353]{Grassmann2000})

Also introduced the combinatorial product (Ch. 3), which we recognize today as the determinant.


\paragraph{B. Peirce (1873)~\cite{Peirce1873} and C. S. Peirce (1874)~\cite{Peirce1874}}

Developed the notion of matrix unit, which he called ``elementary relative'' in \cite[p.359]{Peirce1873}.

B. Peirce (1874)~\cite{Peirce1874} explains that these units, which he called ``vids'', form the basis of a linear algebra. This paper also appears to be the earliest use of the phrase ``linear algebra'' (at least, in English).

In 1883~\cite{Peirce1883} C. S. recognizes the significance for the algebra of matrices, called then by Clifford ``quadrics''.

\paragraph{Frobenius (1878)~\cite{Frobenius1878}}

May have invented the term ``bilinear form''.
The product is similar to the product law of two determinants (matrices?) (p. 346)


\paragraph{Clifford (1878)~\cite{Clifford1878}}

Clifford’s associative geometric product
Clifford 1878 of two vectors simply adds the
(symmetric) inner product to the (antisymmetric) outer product of Grassmann, - does it mean that he used the term also?


\paragraph{Gibbs (1881)~\cite{Gibbs1881}}

Unpublished lecture notes on vectors (see \cite{Wilson1901} for published version).

Introduced notation of small Roman letter in boldfact (``Clarendon type'') for vectors.

The dot product is $\mathbf a \cdot \mathbf b$.
\cite[p. 55]{Wilson1901} introduces the direct product, ``read \textbf{A} \textit{dot} \textbf{B} and therefore may be called the dot product instead of the direct product. It is also called the scalar product owing to the fact that its value is scalar.''.

He acknowledges Grassmann's \textit{Ausdehnungslehre} as containing important concepts that were unnamed. Gibbs decides to call the general product case an ``indeterminate product''.

Gibbs discusses vectors in the case of $n=3$ dimensions only. In this setting the outer product is called a dyad, written with the variables juxtaposed, $\mathbf{ a \; b }$.
Gibbs stresses to think of this as a formal product waiting to act upon a vector

The dyadic or dyadic product is $\mathbf{ a \; b } + \mathbf{ c \; d } + \mathbf{ e \; f }$ and is what we would recognize today as the rank 1 expansion of a $3 \times 3$ matrix in outer products.

TODO Did Gibbs do any of this in the primary literature before this work?

TODO Gibbs 1886 is also worth discussion separately


\paragraph{Weierstrass (1884)~\cite{Weierstrass1884}}

Publishes the notion of defining formal linear algebra through its structure constants. (He does not give the numbers a name.)



\paragraph{Dedekind (1885)~\cite{Dedekind1885}}

Extends Weierstrass to give the restriction on the structure constants so that the resulting algebra is associative. (He does not give the numbers a name either.)



\paragraph{Heaviside (1886)~\cite{Heaviside1886}}

Of significance for systematically introducing boldface small Roman letters (in ``Clarendon type'') to denote vector variables.~\cite{Heaviside1886}. (Heaviside gives a very amusing diatribe against the prevailing custom of Greek latters by Tait and Germanic Gothic letters by Maxwell in \cite[\S 103, pp. 139-142]{Heaviside1894})

TODO did Gibbs use bold face too? When?

\cite[\S 107]{Heaviside1894} defines the scalar product as
\[
\mathbf{AB} = \mathrm{AB} \cos \theta
\]
and calls A and B the ``tensors'' of $\mathbf{A}$ and $\mathbf{B}$. The cross product $\mathbf a \times \mathbf b$ he writes as $\mathbf{V a b}$.

\paragraph{Peano (1888)~\cite{Peano1888}}

Formalized axioms of vector space latent in Grassmann's work.
May have also reintroduced the scalar product?

Peano used [Peano 1887]
for the scalar product, but called it “prodotto di due segmenti” (since in this book vectors
were called ‘segmenti’). He used
again later [Peano 1891a] and called it “prodotto
(interno o geometrico).”



\paragraph{Scheffers (1889)~\cite{Scheffers1889}}

Formal linear algebra. Did he copy Weierstrass?? His paper has the same name.


\paragraph{Taber (1890)~\cite{Taber1890}}

A historical overview of the development of the theory of matrices.
Recognized that matrices were latent in the theory of vectors as developed by Hamilton in his \textit{Lectures on Quaternions}\~cite{Hamilton1852}.
Taber has a sense of outer product and its role in defining matrices (``linear unit operator[s]'') via bilinear forms. In modern language,
a matrix $\rho$ is written in the rank-1 expansion form:
\[
\rho = x_1 \alpha_1 + x_2 \alpha_2 + \dots + x_\omega \alpha_\omega
\]
then the linear unit operator $\phi$ can be expressed as a matrix by considering its action on each basis vector $\alpha$
\[
\phi\rho = x_1.\phi\alpha_1 + x_2.\phi\alpha_2 + \dots + x_\omega.\phi\alpha
\]
which can be used to develop the system of equations
\begin{align}
\phi \alpha_1 & = a_{11} \alpha_1 + a_{21} \alpha_2 + \dots + a_{\omega 1} \alpha_\omega \\
\phi \alpha_2 & = a_{12} \alpha_1 + a_{22} \alpha_2 + \dots + a_{\omega 2} \alpha_\omega \\
              & \cdots \cdots \cdots \\
\phi \alpha_\omega & = a_{1\omega} \alpha_1 + a_{2\omega} \alpha_2 + \dots + a_{\omega \omega} \alpha_\omega \\
\end{align}
thus turning the definition of $\phi$ into a system of linear equations over the basis vectors $\alpha$, when then defines the linear transformation $\phi$ using the matrix of coefficients $(a)_{ij}$ in the system of equations.
(Note that Tabor's indexing is row major, in contrast with modern convention.)
Intriguingly, Tabor also uses a very suggestive notation
\[
\phi\rho = (\phi|x_1)\alpha_1 + (\phi|x_2)\alpha_2 + \dots + (\phi|x_\omega)\alpha
\]
Furthermore, Taber reviews C S Peirce's ``forms'' or ``vids'' $(\alpha_i : \alpha_j)$, and says they are equivalent to ``elementary units'' of a matrix. It is clear fhat Peirce's notation corresponds to $a_j a_i^T$ today.
He introduces Clifford's ``quadrates'' (which are just matrices).
\begin{quote}
I shall therefore call an algebra linear in $\omega^2$ of these vids a quadrate algebra of order $\omega$; and any expression linear in the vids, a \textit{quadrate form}.
\end{quote}
Thus introducing the term that is almost the modern form, a quadratic form.

\paragraph{Molien (1892)~\cite{Molien1892}}

Presents the first systematic treatment of the linear algebra of hypercomplex numbers.
In modern terms, we would say that the algebra is characterized by structure constants $c_{ij}^k$ such that each basis element of the algebra satisfies the multiplicative identities
\[
e_i e_j = \sum_k c^k_{ij} e_k
\]
Molien presents the inverse relation.
\begin{quote}
,,Das distributive Gesetz der Multiplication verlangt, dass das Product
zweier complexer Zahlen eine Zahl sei, deren Parameter sieh als bilineare
Formen darstellen, die mit Hilfe constanter Grössen aus den beiden
Reihen der Parameter der Factoren gebildet sind.``
\end{quote}
For $u, x, x^\prime$ in the hypercomplex number system, the multiplication law
requires
\[
x_i^\prime = \sum_{kl} a^i_{kl} x_k u_l
\]
Molien also uses the word ,,Basis des Zahlensystems`` (basis of the number system).

An important secondary source for the significance of Molien's work is~\cite{Hawkins1972}.

Gibbs and Heaviside (????) - may have introduced the scalar product?



\paragraph{Ricci and Levi-Civita (1900)~\cite{Ricci1900,Hermann1975}}

The famous paper on differential geometry and tensor analysis.

They introduce the tensor product, but simply call it the
«produit des deux systèmes [covariants ou contrevariants]»~\cite[p. 133]{Ricci1900}, translated as
``the (tensor) product of the two (covariant [or contravariant]) tensor fields''~\cite[p. 28]{Hermann1975}.

p. 135 introduces the quadratic form of two 3-dimensional differential elements as a «forme fondamentale $\varphi$[...] en coordonnées générales» (``fundamental form $\varphi$[...] in general coordinates)

\[
\varphi = \sum_1^3 \text{\tiny{rs}} a_{rs} dx_r dx_s
\]

in other words, they spell out the quadratic form in terms of the elements.



\paragraph{Prandtl (1903) and the German Vector commission}

Recommended the notation of $\mathbf{a} \cdot \mathbf{b}$ a la Gibbs but called it the inner product à la Grassmann.



\paragraph{Burali-Forti and Marcolongo (1907-1908) - Per l’unificazione delle notazioni vettoriali.}

Proposed $\mathbf{a}\times\mathbf{b}$ for the scalar product and  $\mathbf{a}\wedge\mathbf{b}$ for the vector product.



\paragraph{Hahn (1911)~\cite{Hahn1911}}

Possibly the earliest mention of the word ,,eigenvektor`` (``eigenvector'') \cite[p. 35]{Hahn1911}.
He introduces eigenvectors as representations of integrated eigenfunctions corresponding to a finite-dimensional integral kernel.



\paragraph{Cullis (1913)}

First monograph on matrices that pays attention to the properties of rectangular matrices, not just square ones.
A and x are their own objects, not just tables of numbers.
Cullis is notable for introducing the modern typographic conventions for types variables. In particular matrices are capital Roman, vectors (as ``horizontal rows'' or ``vertical rows'' of a matrix) are small Roman, and scalars as small Greek appear here.
Another notable notational convention is the use of $A = [a]^m_n$ and the transpose as $A' = \overbracket{\underbracket{a}}^m_n$. The prime in $A'$ is used informally in the sense of ``a quantity derived from A'', but often (if not always) is used to denote the transpose.
The bracket convention for the transpose is worth explaining further in how it works to fill in the symbolic entries...
Another notational convention that is noteworthy is Cullis's use of what we would today call the trailing singleton dimension rule. For column and row matrices, the elements of those matrices are not denoted by $a_{1n}$ or $a_{m1}$, but rather just $a_n$ or $a_m$. In other words, he drops the 1 when convenient. He also does this to the brackets: $x = [x]_n$ is equivalent to $[x]^1$.
Cullis 1928 introduces bilinear forms in notation that is almost recognizable to us today...



\paragraph{Einstein and Grossmann (1913)}

A pamphlet on general relativity, divided into two parts for the physical and mathematical content. In Part II (by Grossmann) we see

\begin{quote}

,Die der gewöhnlichen Vektoranalysis entnommenen Bezeichnungen ,,äußeres und inneres Produkt`` rechtfetigen sich, weil jene Operationen sich letzten Endes als besondere Fälle der hier betrachteten ergenen.`~\cite[p. 26]{Einstein1913}\cite[p. 327]{Einstein1995}

`The designations ``inner and outer product'', which are taken from ordinary vector analysis, are justified because, when all is said and done, those operations prove to be special cases of the operations considered here.'~\cite[p. 175]{Einstein1996}
\end{quote}
The name ``outer product'', however, appears to be original. This pamphlet references only \cite{Ricci1900}, but that paper only contains the term ``produit''.
The borrowing of the term ``outer product'' may be responsible for the conflation of the two meanings in present usage, as later English books about relativity consistently use one meaning or the other. either meaning. For example,

\cite[p. 25]{Murnaghan1922} uses ``outer product'' in Grassmann's sense, ``direct product'' for tensor product, and ``inner product'':
\begin{quote}
``The difference $X_{rs} - \overline X_{rs}$ [...] should be more important than either of the direct products $X_{rs}$ [$\equiv X_r \cdot \overline X_s = X \cdot \overline X$] or $\overline X_{rs}$. It is what Grassmann called the \textit{outer product} of the two tensors $X, \overline X$.''
\end{quote}

\cite[p. 87]{Carmichael1920} uses ``outer product'' for the tensor product and the ``inner product'' in line with Einstein and Grossmann:
\begin{quote}
``[W]e used the term product of two tensors to denote the tensor whose component elements are all the elements formed by multiplying an element of one tensor by the element of another tensor. This may be called their \textit{outer product}. We also need the notion of \textit{inner product} of two vectors, say of $A_\mu$ and $B^\mu$; and this is defined to be the quantity $\sum_\mu A_\mu B^\mu$; that is, the sum of products of corresponding elements.''
\end{quote}
(These terms are absent in his first edition of 1913)

\cite{Silberstein1922} is interesting for the mention of the ``outer product of two vectors'', and also discussing the inner product as being derived from the contraction of the outer product.
\begin{quote}
(\S 15, p. 43) Consider, on the other hand, what is known as \textit{the outer product} of two vectors, of the same or of opposite kinds, \textit{i.e.}, $A_\iota B_\kappa$, or $A^\iota B^\kappa$, or $A_\iota B^\kappa$.

(\S 18, p. 48) \textit{The inner multiplication}, already meThe outer multiplication combined with contraction (when there are indices to contract) gives the inner product. Thus the inner product of $A_\iota$ and $B^\kappa$ is
\[
A_\kappa B^\kappa = M^\kappa_\kappa = M,
\]
an invariant. (There is no inner product of $A_\iota$, $B_\kappa$.)
\end{quote}



\paragraph{Courant (1920)}

Next earliest use of the word ,,eigenvektor`` (after Hahn, 1911).
In, \S 10 he specifically talks about "vectorial eigenvalue problems". He writes

\begin{quote}
,,Man kann ferner analoge Eigenwertprobleme untersuchen, bei denen an Stelle der gesuchten Funktion $u$ ein Vektor $u$ steht, welcher ebenfalls am Rande gewissen homogenen Randbedingungen unterworfen ist.``

``One can further examine analogous eigenvalue problems, where instead of the unknown function $u$ a vector $u$ stands in, which is also subject to certain homogeneous boundary conditions on the edge.''
\end{quote}

Clearly the implication here is that eigenvalue problems have been traditionally about eigenfunctions, not eigenvectors.



\paragraph{Hilbert and Courant (1924)}

The monumental work on mathematical physics. Apparently the first systematic treatment of matrix algebra for the physics audience?



\paragraph{Born and Jordan (1925)~\cite{Born1925}}

Explains quantum mechanics as matrix mechanics. First major work to introduce matrices into the physics literature.

They are familiar with the notions of vector analysis, but mainly in 3 and 4 dimensions.

,,skalare Produkt`` $(\mathfrak A, \mathfrak B)$ (p. 885)



\paragraph{ Van der Waerden (1930)}

Credited in \cite{Kleiner2007} as the origin of the term "Linear algebra". But see
Peirce (1874).



\paragraph{Weyl (1931)~\cite{Weyl1931}}

Introduces a vector $\mathfrak x$ as ``a set of $n$ ordered numbers $(x_1, x_2 \cdots, x_n)$.

The ``inner product... can be written in matrix notation as''
$\xi^*x$ or $x^*\xi$.

A very curious thing: "outer or $\times$ m[ultiplication] of spaces, vectors, operators" shows up in the index of the English translation, but the word ``outer'' is absent on the cross-referenced pages both in the English and German editions. The original German editions don't have an entry with ``outer'' at all. So we must conclude that this was an invention of the English translator, who is himself a notable American physicist.


\paragraph{Turnbull and Aitken (1932)}

Emphasis on systematic notation for quadratic and bilinear forms as $x'Ay$.
To be distinguished from their earlier 1926 book, which does not adopt this notational convention.
They credit Cullis for his emphasis on the algebra of rectangular matrices. Although they do not credit him also for the notation, it is clear that his typographic conventions were adopted in their exposition.



\paragraph{MacDuffee (1933)~\cite{MacDuffee1933}}

preface mentions bilinear and quadratic forms (not sure if book does)
probably quoting eisenstein

"Denote by (x) the vector or one-column array" - p10

"A matrix is often considered as a linear vector function." - p16

"A matrix may be interpreted as a linear homogeneous transformation in
vector space. From this point of view similar matrices represent the
same transformation referred to different bases. All the theorems of
this chapter may be interpreted from this standpoint." - p68

Starts with the abstract axioms of linear algebra and shows that the algebra may be represented by naturally by matrices and matrix multiplication. Also defines array as an ordered set with a known number of elements.

\paragraph{Wedderburn (1934)~\cite{Wedderburn1934}}

Starts with vectors as ordered sets of numbers as matrices as collections of coefficients in systems of linear equations.

Bilinear form $A(x, y) = \sum^n_{i,j=1} a_{ij} \xi_i \eta_j $ (\S 1.07, p. 9).

The scalar product is a particular bilinear form, $S x y$ (\S 1.07, p. 9), ``where $A = \Vert \delta_{ij} \Vert = 1$''. He also extends the scalar product to matrices (\S 5.02, p. 62), i.e. what we call today the Hadamard product, and also to tensors (\S 5.16, p. 81).

Using the $S$-prefix notation for scalar product he also writes the bilinear form as $A(x, y) = SxAy$.

He calls the outer product a ``tensor product'' (\S 5.09, pp. 72-73).

Alternates between "vectors"
and "hypernumbers". On p72, Sec 5.09, he defined "vector products" as
"tensor product". He attributed in Footnote 3 that Grassmann invented
this product as the "general product" or "indeterminate product".

Follows \cite{Scheffers1889}.



\paragraph{King (1934)}

A chemistry paper from 1934 using the term "outer product" in
our modern usage. which notes that it is closely related to both Kronecker products on
matrices and the indeterminate product (dyad) of vectors. All the
citations in [1] refer to Kronecker products, with the original
citation being Zehfuss, 1858. [3] refers to Gibbs's terminology of
dyad, also indeterminate product.

\paragraph{Schwerdtfeger (1938)~\cite{Schwerdtfeger1938}}

A book about matrix functions.
Vectors come first as «$n$-uple»s (``$n$-tuples'') that represent points. \S 2, pp. 1-2, which discusses change of coordinates, he writes
\begin{quote}
«$y_1, \dots, y_n$ serait un autre $n$-uple par lequel on pourrait représenter le point $x$. Dans cette conception, le point x aurait le caractère d'un vecteur.»

``$y_1, \dots, y_n$ would be another $n$-tuple by which one could represent the point $x$. In this design, the point $x$ would have the character of a vector.''
\end{quote}

In \S 3, p. 2, he introduces the representations of points «par des symboles de colonnes» (``by columns of symbols'')
\[
x =\begin{pmatrix}x_1\\x_2\\:\\x_n\end{pmatrix}
\]
and a $n \times p$ matrix is introduces as «un système de $p$ colonnes» (``a system of $p$ columns'')

Here we also see the transposition of a vector, although it is not described as such.
\begin{quote}
«La matrice à une ligne, ayant les mêmes coordonnées que la matrice à une colonne $x$, sera désignée par

\[
x^\prime = \begin{pmatrix}x_1 & x_2 & \cdots & x_n\end{pmatrix}
\]

En enployant encore une fois cette opération de ``\textit{transposition}'' on sera ramenée au point $x$ ($= x^{\prime\prime}$).»

``
The matrix with one line, having the same coordinates at the matrix with one column $x$, will be designated by[...]

Employing once again this operation of ``transposition'' one will reduced to the point $x$ ($= x^{\prime\prime}$).''

\end{quote}
Here we see the notion that the transpose is idempotent, i.e. $x = x^{\prime\prime}$.
In \S 8 we see mention of Le «\textit{produit intérieur} (ou \textit{scalaire})» as defined as the number $x^\prime y$ defined in \S 3. Le «\textit{produit extérieur}» refers to the cross product and there is a discussion of the representation of the exterior product in the algebra of skew-symmetric matrices.

Citations:

von Neumann, Allgemeine Eigenwerttheorie Hermitesche Funktionaloperatoren
Math Ann 201 1929 49-131 (A = A' as projection operators)

cites the use of fundamental notions of modern algebra and algebraic numbers

Hasse, Hohere Algebra I, Samml  Goeschen 931, 1926

O Ore, Les corps algébriques et al théorie des idéaux, Mémorial des Sc. Math 64 1934


\paragraph{Aitken (1939)~\cite{Aitken1939}}

Matrices first. Column and row vectors defined synonymously with their matrix equivalents.
of orders $n\times1$ and $1\times p$.
column vectors are $x = \{x_1\;x_2\;\dots\;x_n\}$,
row vectors are $u = [u_1\;u_2\;\dots\;u_p]$.

\S 4, p. 5

\S 10 - bilinear, qudratic and Hermitian forms $x^\prime A y$.

No mention of inner or outer products.


\paragraph{Margenau \& Murphy~\cite{Margenau1943}}

An influential textbook ``Mathematics of Physics and Chemistry'' defines arrays
effectively as matrices without linear algebra~\cite[\S 10.1, p. 288]{Margenau1943}:

\begin{quote}
A collection of real or complex quantities is called an
\textit{array} if it can be displayed in an orderly table of \textit{rows} and \textit{columns}.
\end{quote}

They consider row and column vectors as matrices as determinants as a particular kind of array.

A matrix is typeset in bold italic capital Roman, $\boldsymbol A$.
A vector is typeset in bold small Roman, $\textbf u$ or enclosed in braces, $\{ \textbf u \}$.
Row are $[\textbf u ]$.
Kronecker product of matrices is the direct product.
Transpose is $\tilde{\boldsymbol A}$.
Scalar product (10-11), $\tilde{\mathbf x} \mathbf y$ or $[\mathbf x] \{\mathbf y\}$ (10-7).
Hermitian scalar product is $\mathbf x^\dagger \mathbf y$



(Notes above are for second edition)



\paragraph{Wade (1948)~\cite{Wade1948}}

Cited in second edition of \cite{Margenau1943}.

Vectors come first as $n$-tuples with vector space structure.
Vectors are $\alpha$, matrices are $A = [a]^m_n$
inner product is $\alpha \cdot \beta$, synonymous with dot product, scalar product, direct product.
outer product and forms absent.
in Ch. 5, matrices, vectors are introduces as constitutents of matrices
\S 5.4 p 36 - Matrices as ordered sets of row vectors (primarily) or column vectors (secondarily).
\S 5.5 p 37 $A = [a]^1_n = (a_1, a_2, \dots, a_n)$ is a row matrix,
$A = [a]^m_1 = \{a_1, a_2, \dots, a_m \}$ is a column matrix. (Also appears vertically)
\S 5.5 p 38 - matrix transpose is $A^\prime$.
\S 5.5 p 39 - the transpose of a column vector is a row vector.
\S 5.5 p 39 - Column vectors are $\alpha$.



\paragraph{Ritt (1950)~\cite{Ritt1950}}

First use of the term ``structure constant'' ?

\url{http://www.jstor.org/stable/1969444?seq=1#page_scan_tab_contents}


\paragraph{Perlis (1952)~\cite{Perlis1952}}

Matrices first. Column and row vectors defined synonymously with their matrix equivalents.

Column vectors are $X = \textrm{col }(x_1, \dots, x_s)$ - p. 4, \S 1.2.
\S 1-4, p. 9 - scalar product is scalar-matrix product, $c A$.

Vector spaces come in Ch. 2. Vectors as $n$-tuples. are small Greek letters.

\begin{quote}
Thus ordered $n$-tuples will be regarded here as objects which may be represented as rows, or columns, whichever may be convenitn at the moment.

p. 24, \S2-2.
\end{quote}


\paragraph{Householder (1953)~\cite{Householder1953}}

The first book on linear algebra focusing on numerical algorithms.

Householder is famous for insisting on a systematic notation for various quantities.
The convention for matrix variables written in capital Roman letters, vectors in small Roman letters, and scalars in small Greek letters was popularized by him. Note however that the works of Cullis already used such a systematic convention.
There is also the superscript $T$ for transpose, with expressions like $x^T y$ and $1 - x y^T$.
Although Householder never uses the term ``outer product'', we see expressions like $D - u v^T$ in the book, so we see diagonal plus rank one terms everywhere. He does not present $u v^T$ as an expression deserving of its own identity.


\paragraph{Faddeev and Faddeeva - \cite{Faddeev1959}}

Matrices first.

\paragraph{Schreier and Sperner (1959)~\cite{Schreier1959}}

Vectors first.
Vectors are fraktur, $\mathfrak a = \{a_1, a_2, \cdots, a_n\}$
Scalar product $\mathfrak a \cdot \mathfrak b$. - end of \S 2

\S6 Associate a vector with each column of [a] matrix - \S 6

No mention of forms, inner product or outer product.

TODO Original first edition requested. (1931)

\paragraph{Iverson (1962)~\cite{Iverson1962}}

Possibly noteworthy as the first publication of a computer program for numerical linear algebra using vectorized operations, as implemented in APL.
Resurrects (?) the outer product in a paper describing the implementation of Gauss-Jordan elimination using vectorized APL operations. The description of Program 2 (and its accompanying footnote) states that

\begin{quote}
`[S]tep 8 subtracts from $\underline{\textit{M}}$ the outer product of the first row (except that the first element is replaced by zero) with its first column. The outer product $\underline{\textit{Z}}$ of a vector $\underline{\textit{x}}$ by vector $\underline{\textit{y}}$ id the ``column by row product'' denoted by $\underline{\textit{Z}} \leftarrow \underline{\textit{x}} \times \underline{\textit{y}}$ and defined by $\underline{\textit{Z}}^{\; i}_j = \underline{\textit{x}}_i \times \underline{\textit{y}}_j$.'
\end{quote}

This paper cites an ALGOL implementation of Gauss-Jordan elimination, but this algorithm never computes the outer product explicitly, instead computing the outer product on the fly as part of the update of the matrix:~\cite{Cohen1961}

\begin{quote}
\begin{verbatim}
a[k,j] := a[k,i] - b[j] x c[k]
\end{verbatim}
\end{quote}

Iverson does not claim that his APL version is equivalent to the ALGOL one, but none of his references ever refer to the $b c^\prime$ matrix as the outer product of vectors.

It is fair to say that by 1962, Iverson recognized the quantity $b c^\prime$ as having its own existence outside of the diagonal+rank1 expression $I - b c^\prime$, and that he did so independently of anyone else. In this paper and the APL book Iverson meant the notation to be useful for both machine computation and analysis by hand, so it is unclear if Iverson ever intended $b c^\prime$ to be constructed explicitly on a machine. (It seems unlikely given the context of matrix inversion.) Whether or not anyone else noticed before that is debatable - Rutishauser didn't in 1959.

(Curiously, the APL book does not mention the phrase ``outer product'', despite having it in the index!~\cite{Iverson1962book})


\paragraph{Wilkinson's "The algebraic eigenvalue problem", 1964}

Did he have anything notationally significant?



\paragraph{Forsythe and Moler (1967)~\cite{Forsythe1967}}

They present the vector transpose on p. 2.
\begin{quote}
``Let $x = (x_1, x_2, \dots, x_n)^T$ denote a column vector in real $n$-dimensional space $R^n$. Let $x^T$ denote the row vector which is the transpose of $x$.''~\cite[p. 2]{Forsythe1967}
\end{quote}

Although they cite \cite{Faddeev1959} for the introductory material, \cite{Faddeev1959} itself is firmly rooted in the ``matrices only'' tradition, with column vectors treated as synonymous with column matrices.




\paragraph{Strachey (1967)~\cite{Strachey1967}}

By the time computer scientists thought of formalizing the grammar and semantics of programming languages,
vectors and arrays are considered well-understood data structures. \cite[\S 3.7.7, pp. 43--44]{Strachey1967} writes:
\begin{quote}
Vectors and arrays are reasonably well understood. They are parametric types so that the type of an array includes its dimensionality (the number of
its dimensions but not their size) and also the type of its elements[...]
all the elements of an array have to be of the same type, though their number may vary dynamically.
It is convenient, though perhaps not really necessary, to regard an $n$-array
(i.e., one with $n$ dimensions) as a vector whose elements are ($n-1$)-arrays.
We can then regard the R-value of a vector as something [that] gives access (or points to) the elements rather than containing them.
\end{quote}
We recognize herein the modern description of the array data type being defined by
two parameters: the number of dimensions and the type of elements.

\cite{Landin1965}:
\begin{quote}
An array is considered as a function whose domain is a subset of the set of integer-lists.
It is initialized with the appropriate domain (not subsequently altered) and with all of its elements equal.
\end{quote}

TODO Requested: secondary references in "History of Fortran i-3" by Backus

Moser - A-2 compiler (1954) and one other thing

mention of array o subscript variables?

TODO TAoCP v3 credits Von Neumann with inventing merge sort. Did he also invent the array?

TODO De Bakker 1967 requested; does it mention vectors as data types?

Algol 1958 - array variables may be multidimensional, must correspond to subscripted variable with as many empty parameter positions as dimensions.
In other words, the dimensionality of an array in Algol is crucial to determine how many indexes to supply.

PL/I
\begin{quote}
Since it appears to be the
easiest way during interpretation to handle data aggregates by recursively defined
functions or instructions level by level, the structuring of data attributes is
described level by level :

(a)
Arrays. A multi-dimensional array is decomposed into a nested sequence of
one-dimensional arrays; e.g. a two-dimensional array of scalars is handled
as a one-dimensional array, whose elements are themselves one-dimensional
arrays of scalars. An array of structures or cells is naturally handled in
the same way with the only difference that its base elements are described
as structures or cells . so , an abstract program has only one-dimensional
array data attributes; the elements may be arrays , structures , cells or
scalars.

Array data attributes consist of three components: An expression denoting
·the lower bound (if missing in the concrete program, the constant 1 is inserted
by the translator) , an expression denoting the upper bound and the
data attributes of the elements of the array

--- \cite[\S 2.3.1, pp. 2-8--9]{Lucas1968}
\end{quote}



\section{Classficiation of the four main schools of vectors and matrices}

\begin{tabular}{l}
Mostly matrices \\
\hline
Cullis (1913-28) \\
\end{tabular}

\begin{tabular}{l}
Mostly vectors \\
\hline
Grassmann (1842-1866) \\
\end{tabular}

\begin{tabular}{l}
Matrices are primary \\
\hline
Macduffee (1933) \\
Fadeev and Faddeeva (1959) \\
\end{tabular}


\begin{tabular}{l}
Vectors are primary \\
\hline
\end{tabular}

\section{Other quotes Alan likes}

\begin{quote}
``In linear algebra, better theorems and more insight emerge if complex numbers are investigated along with real numbers.''-\cite[p. 1]{Axler2015}
\end{quote}
