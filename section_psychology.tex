\label{sec:householder}
\section{User psychology}

\begin{quote}
``In linear algebra, better theorems and more insight emerge if complex numbers
are investigated along with real numbers.''-\cite[p. 1]{Axler2015}
\end{quote}

The goal of this section is to substantiate the ``what users want'' part of our
thesis. While seemingly obvious to most practitioners today, the historical
evidence for writing expressions using compositions of transposition and
products has been most illuminating and reflects the multifaceted evolution of
what we know today as linear algebra.

Programming languages designed for scientific computing have many users which
are familiar with linear algebraic constructs expressible using various
products and quotients involving matrix and vector quantities. Some of these
quantities include:

\begin{itemize}
  \item Inner products, $u^\prime v$
  \item Outer products, $u v^\prime$
  \item Bilinear forms, $u^\prime A v$
  \item Quadratic/Hermitian forms, $v^\prime A v$
\end{itemize}

The notation for these quantities were systematized by Householder
\cite{Householder1953,Householder1955}, who used capital Roman letters for
matrices, small Roman letters for (column) vectors, and small Greek letters for
scalars. Importantly, Householder also uses the vector transpose notation
$u^\prime$ to express row vectors, which are then used to construct the above
products and forms. \cite{Householder1955} uses $u^T = {(u_1, \cdots, u_n)}^T$ to
express row vectors as tuples, a construct which we shall see later as having
ample precedent. \cite{Householder1955} writes the inner product as $u^* v$,
noting that $*$ ``denotes the conjugate transpose''. \cite[Sec.
4.01]{Householder1953} introduces the Hermitian form. \cite[Sec.
2.04]{Householder1953} introduces the row vector $u^T$ corresponding to the
column vector $u$. Householder also acknowledges in \cite[Sec.
2.04]{Householder1953} the analogy between a numerical vector (as a column of a
matrix) and a geometric vector in the sense of Gibbs. He writes the vector
transpose as if it follows immediately from the matrix transpose:

If each column of a matrix $M$ is written as a row, the order remaining the
same, the resulting matrix is known as the transpose of the original and is
designated $M^T$. In particular, if $x$ is the column vector of the $\xi_i$,
$x^T$ is the row vector of the $\xi_i$.

The outer product shows up in \cite[Sec. 2.24]{Householder1953}, although
Householder does not give the quantity $u v^T$ a special name\footnote{while
\cite[Sec. 2.03]{Householder1953} introduces ``outer products'' $[u v]$, these
quantities are known today as bivectors and are conventionally denoted $u
\wedge v$.}, and the discussion in context clearly implies that Householder
considers vector-scalar-vector products $u \sigma v^T$ special cases of matrix
products $U S V^T$.

Interestingly, \cite{Householder1953} does not use the terms ``bilinear form'',
``quadratic form'', or ``Hermitian form''. He uses ``scalar product''.
\cite{Householder1955} clearly spells out his notational convention.
(\cite{Householder1953} has a missing page which might also spell it out, but
it's not clear.)



\subsection{More complicated expressions}

These basic expressions can be used to build more complicated expressions that
occur in scientific computing.

Examples of more complicated expressions

BFGS Update Step

Alan's example with least squares? MANOVA/Jacobi distribution formulas?

\section{Householder notation assumes implicit embeddings between scalars,
vectors and matrices.}

This notation is what we associate today with modern linear algebra.
We have
\begin{description}
\item [{Inner~product}] $u^{T}v$, a scalar,
\item [{Outer~product}] $uv^{T}$, a matrix,
\item [{Quadratic~form}] $u^{T}Au$, a scalar,
\item [{Bilinear~form}] $u^{T}Av$, a scalar,
\end{description}
from which a wide variety of expressions like Rayleigh quotients $u^{T}Au/u^{T}u$
can be constructed.

PEP 465 gives the example $\left(H\beta-r\right)^{T}\left(HVH^{T}\right)\left(H\beta-r\right)$,
which is part of the expression for the $F$ test statistic in hypothesis
testing. This fomula is a quadratic form composed of matrix–vector
and matrix–matrix products.

The trick here is that $u$ and $v$ in these expressions are what
Householder called ``numerical vectors'', to be distinguished from
``geometrical vectors''. Householder implicitly assumes a few embeddings,
\begin{enumerate}
\item A $1\times1$ matrix is a scalar. Give this embedding a name, $\theta:F^{1\times1}\rightarrow F$.
\item An $n$-vector is an $n\times1$ matrix. Give this embedding a name,
$\phi:F^{n}\rightarrow F^{n\times1}$.
\end{enumerate}
These assumptions are ubiquitous in the linear algebra literature.
Only a few authors (like Gantmacher) take care to distinguish them.

Now given these embeddings, and the identity embedding $I:F^{m\times n}\rightarrow F^{m\times n}$,
we can derive meanings for all these products from the matrix–matrix
product $*_{mm}:F^{m\times k}\times F^{k\times n}=F^{m\times n}$,
the matrix transpose $\cdot^{T}:F^{m\times n}\rightarrow F^{n\times m}$,
and a suitable choice of embeddings:
\begin{itemize}
\item Inner product $u^{T}v$ is the composition of $\theta\circ*_{mm}\circ\left(\cdot^{T}\times I\right)\circ\left(\phi\times\phi\right)$
acting on $(u,v)$.
\item Outer product $uv^{T}$ is the composition of $*_{mm}\circ\left(I\times\cdot^{T}\right)\circ\left(\phi\times\phi\right)$
acting on $(u,v)$.
\item Quadratic form and bilinear form $u^{T}Av$ are the composition of
$\theta\circ*_{mm}\circ\left(\cdot^{T}\times*_{mm}\right)\circ\left(\phi\times I\times\phi\right)$
thinking of $u^{T}\left(Av\right)$, and thinking of $\left(u^{T}A\right)v$,
is the composition $\theta\circ*_{mm}\circ\left(*_{mm}\times I\right)\circ\left(\cdot^{T}\times I\times I\right)\circ\left(\phi\times I\times\phi\right)$,
both acting on $(u,A,v)$. Thus we must have the identity $*_{mm}\circ\left(\cdot^{T}\times*_{mm}\right)=\left(*_{mm}\times I\right)\circ\left(\cdot^{T}\times I\times I\right)$.
\end{itemize}
In these expressions, $\circ$ is function composition and $a\times b$
describes a sequence of operations $a$ and $b$ to be applied left
to right on a sequence of operands. $(I\times I)(A,B)$ means to apply
the identity map to $A$ and $B$ separately, producing the result
$(A,B)$. $(I\times*)(A,B,C)$ means to apply the identity map to
$A$ and the $*$ to the next two operands $B$ and $C$ (since $I$
is monadic and $*$ is dyadic), producing the result $(A,BC)$.

Suppose we were to try defining the vector transpose $\cdot_{v}^{T}:F^{m}\rightarrow F^{m}$
as the identity map and introduce also a special vecvec product, $*_{vv}:F^{m}\times F^{m}\rightarrow F$,
the matvec $*_{mv}:F^{m\times n}\times F^{n}\rightarrow F^{m}$, and
the vector identity map, $I_{v}:F^{m}\rightarrow F^{m}$. Languages
like Python/NumPy use this definition. If we now consider only the
elementary operations $\cdot_{v}^{T}$, $*_{vv}$, $*_{mv}$, $I_{v}$
and $I$, then we have that
\begin{itemize}
\item Inner product $u^{T}v$ is the composition of $*_{vv}\circ\left(\cdot_{v}^{T}\times I_{v}\right)=*_{vv}\circ\left(I_{v}\times I_{v}\right)=*_{vv}$.
\item Outer product $uv^{T}$ cannot be expressed as any composition of
$*_{mv}$, $*_{vv}$, $\cdot_{v}^{T}$ and $I_{v}$.
\item Quadratic form and bilinear form $u^{T}Av$ are the composition of
$*_{vv}\circ\left(\cdot_{v}^{T}\times*_{mv}\right)$ when thought
of as $u^{T}\left(Av\right)$, and when thought of $\left(u^{T}A\right)v$,
must be $*_{vv}\circ\left(*_{vm}\times I_{v}\right)\circ\left(\cdot_{v}^{T}\times I\times I_{v}\right)$.
\end{itemize}
Note that the associativity constraint associated with bilinear forms
requires the vector-matrix product $*_{vm}:F^{m}\times F^{m\times n}\rightarrow F^{n}$
to be defined.

We can go through the exercise of finding out if the set of elementary
operations $*_{vm}$, $*_{mv}$, $*_{vv}$, $\cdot_{v}^{T}$, $I$
and $I_{v}$ have are equivalent to the set $\phi$, $\theta$, $I,$
$*$ and $\cdot^{T}$. (??? Bad phrasing) The answer is no. While
we can express $*_{vv}=\theta\circ*\circ\left(\cdot^{T}\times I\right)\circ\left(\phi\times\phi\right)$,
$*_{mv}=\phi^{-1}\circ*\circ\left(I\times\phi\right)$, $I_{v}=\phi^{-1}\circ I\circ\phi$
and $*_{vm}=\phi^{-1}\circ*\circ\left(\phi\times I\right)$,  note
however that $\cdot_{v}^{T}$ is now ambiguous. We cannot write $\cdot_{v}^{T}=\phi^{-1}\circ\cdot^{T}\circ\phi$
in general since for any vetor of length $m\ne1$, $\phi^{-1}$ cannot
be applied to the result of $\cdot^{T}\circ\phi$, which is a $1\times m$
matrix. Instead, $\cdot_{v}^{T}=I_{v}=\phi^{-1}\circ I\circ\phi$,
so that vector transpose is now fundamentally diffeent from the matrix
transpose $\cdot^{T}$.

Take now a third choice of elementary operations: $*_{mv}$ and $I_{v}$
from above, but use now $\cdot_{v}^{\tilde{T}}:F^{m}\rightarrow\tilde{F}^{m}$,
mapping to a new thing, row vectors. Introduce also the row-vector–column-vector
product $*_{\tilde{v}v}:\tilde{F}^{m}\times F^{m}\rightarrow F$ and
the column-vector–row-vector product $*_{v\tilde{v}}:F^{m}\times\tilde{F}^{n}\rightarrow F^{m\times n}$.
Then we can say that
\begin{itemize}
\item Inner product $u^{T}v$ is the composition of $*_{\tilde{v}v}\circ\left(\cdot_{v}^{\tilde{T}}\times I_{v}\right)$
starting from $u\times v$.
\item Outer product $uv^{T}$ is the composition of $*_{v\tilde{v}}\circ\left(I_{v}\times\cdot_{v}^{\tilde{T}}\right)$
starting from $u\times v$.
\item Quadratic form and bilinear form $u^{T}Av$ are trickier. If we group
the operations by $u^{T}\left(Av\right)$ then they are the composition
of $*_{\tilde{v}v}\circ\left(\cdot_{v}^{\tilde{T}}\times*_{mv}\right)$
starting from $u\times\left(A\times v\right)$. To define the order
$\left(u^{T}A\right)v$ to yield the same result, we need a new elementary
operation $*_{\tilde{v}m}:\tilde{F}^{m}\times F^{m\times n}\rightarrow\tilde{F}^{n}$
for the row-vector–matrix product, so that we can write $*_{\tilde{v}v}\circ\left(*_{\tilde{v}m}\times I_{v}\right)$.
In practice, however, expressions written in the vast majority of
programming languages implicitly assume one grouping or the order,
depending on whether operators are evaluated left-to-right or right-to-left.
\end{itemize}
Note now that $\cdot_{v}^{\tilde{T}}$ and $*_{\tilde{v}m}$, being
the elementary operations that produce row vectors, cannot be expressed
as any combination of the elementary matrix operations $\phi,$ $\theta$,
$I,$ $*$ and $\cdot^{T}$. However we now can no longer insist on
the equivalence of indexing and taking inner products with a set of
basis vectors, we now have since for the inner product $\cdot=*_{\tilde{v}v}$,
the expression $\left(x^{\prime}\right)_{i}=x\cdot e_{i}$ is now
the result of the operation $*_{\tilde{v}v}$ on $x^{\prime}\times e_{i}$.
A true inner product on $\tilde{F}^{m}\times F^{n}$ should be defined
by $x\cdot y=\sum_{i}x_{i}^{*}y_{i}$, so the result is the complex
conjugate of the same indexing operation on $x$.

To summarize,
\begin{itemize}
\item If you have ``matrix first'' semantics $\phi$, $\theta$, $I,$
$*_{mm}$ and $\cdot^{T}$, then indexing a vector requires trailing
singleton dimensions or linear indexing rules or the embedding $\phi^{-1}$.
You also need the embeddings $\phi$ and $\theta$.
\item If you have ``vector first'' $*_{vm}$, $*_{mv}$, $*_{vv}$, $*_{mm}$,
$\cdot_{v}^{T}$, $I$ and $I_{v}$, then you have a ``unphysical''
vector-matrix product $*_{vm}$, and you cannot express the outer
product $uv^{T}$ using these semantics.
\item If you have ``vector and dual vector'' semantics $*_{\tilde{v}v}$,
$*_{v\tilde{v}}$, $*_{\tilde{v}m}$, $*_{mv}$, $*$, $\cdot_{v}^{\tilde{T}}$
and $I_{v}$, you need the extra operations $*_{v\tilde{v}},*_{\tilde{v}m}$
for the outer product and guaranteeing associativity. However, you
have to define row vectors as a new type of object, and you have to
define  how to index row vectors. Now arrays are no longer uniquely
mapped to just (column) vectors, but they can also be mapped to row
vectors. Not only do you now have a parallel universe of row vectors,
but the equivalence with the inner product operation $*_{\tilde{v}v}$
requires complex conjugation for nonreal $F$, which some may find
strange. Furthermore an indexing operation like $A[:,1]$ now returns
a column vector, which is a different type from $A[1,:]$, a row vector.
Users are forced to deal with the complexity of intermixing row and
column vectors. (TODO SHOULD INTRODUCE EMBEDDINGS THAT PRODUCE TRAILING
SINGLETONS. INSIST THAT EMBEDDINGS CHANGE THE NATURE OF INDEXING.)
\end{itemize}
This section was about defining platonic ideal semantics. In a later
section we will see how the real world of implementaiton in programming
languages complicates the picture, namely the limitations of working
only at the individual operator level and not at the whole-expression
level.
